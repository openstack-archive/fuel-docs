<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Create a multi-node OpenStack cluster using Fuel &mdash; Fuel for OpenStack 3.0 documentation</title>
    
    <link rel="stylesheet" href="../_static/sphinxdoc.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '3.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <link rel="top" title="Fuel for OpenStack 3.0 documentation" href="../index.html" />
    <link rel="next" title="Production Considerations" href="0055-production-considerations.html" />
    <link rel="prev" title="Create a multi-node OpenStack cluster using Fuel Web" href="0045-installation-fuel-web.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="0055-production-considerations.html" title="Production Considerations"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="0045-installation-fuel-web.html" title="Create a multi-node OpenStack cluster using Fuel Web"
             accesskey="P">previous</a> |</li>
        <li><a href="../index.html">Fuel for OpenStack 3.0 documentation</a> &raquo;</li> 
      </ul>
    </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
  <h3><a href="../index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Create a multi-node OpenStack cluster using Fuel</a><ul>
<li><a class="reference internal" href="#how-installation-works">How installation works</a></li>
<li><a class="reference internal" href="#before-you-start">Before You Start</a></li>
<li><a class="reference internal" href="#infrastructure-allocation-and-installation">Infrastructure Allocation and Installation</a><ul>
<li><a class="reference internal" href="#software">Software</a></li>
<li><a class="reference internal" href="#network-setup">Network setup</a></li>
<li><a class="reference internal" href="#physical-installation-infrastructure">Physical installation infrastructure</a></li>
<li><a class="reference internal" href="#virtual-installation-infrastructure">Virtual installation infrastructure</a><ul>
<li><a class="reference internal" href="#configuring-virtualbox">Configuring VirtualBox</a></li>
<li><a class="reference internal" href="#creating-fuel-pm">Creating fuel-pm</a></li>
<li><a class="reference internal" href="#creating-the-openstack-nodes">Creating the OpenStack nodes</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#installing-configuring-fuel">Installing &amp; Configuring Fuel</a><ul>
<li><a class="reference internal" href="#installing-fuel-from-the-iso">Installing Fuel from the ISO</a></li>
<li><a class="reference internal" href="#configuring-fuel-pm-from-the-iso-installation">Configuring fuel-pm from the ISO installation</a></li>
</ul>
</li>
<li><a class="reference internal" href="#installing-the-os-using-fuel">Installing the OS using Fuel</a><ul>
<li><a class="reference internal" href="#configuring-cobbler-with-config-yaml">Configuring Cobbler with config.yaml</a></li>
<li><a class="reference internal" href="#loading-the-configuration">Loading the configuration</a></li>
<li><a class="reference internal" href="#installing-the-operating-system">Installing the operating system</a></li>
</ul>
</li>
<li><a class="reference internal" href="#generating-the-puppet-manifest">Generating the Puppet Manifest</a></li>
<li><a class="reference internal" href="#understanding-the-puppet-manifest">Understanding the Puppet Manifest</a><ul>
<li><a class="reference internal" href="#enabling-neutron">Enabling Neutron</a></li>
<li><a class="reference internal" href="#defining-the-current-cluster">Defining the current cluster</a></li>
<li><a class="reference internal" href="#enabling-cinder">Enabling Cinder</a></li>
<li><a class="reference internal" href="#enabling-glance-and-swift">Enabling Glance and Swift</a></li>
<li><a class="reference internal" href="#configuring-openstack-to-use-syslog">Configuring OpenStack to use syslog</a></li>
<li><a class="reference internal" href="#setting-the-version-and-mirror-type">Setting the version and mirror type</a></li>
<li><a class="reference internal" href="#setting-verbosity">Setting verbosity</a></li>
<li><a class="reference internal" href="#configuring-rate-limits">Configuring Rate-Limits</a></li>
<li><a class="reference internal" href="#enabling-horizon-https-ssl-mode">Enabling Horizon HTTPS/SSL mode</a></li>
<li><a class="reference internal" href="#defining-the-node-configurations">Defining the node configurations</a></li>
<li><a class="reference internal" href="#installing-nagios-monitoring-using-puppet">Installing Nagios Monitoring using Puppet</a><ul>
<li><a class="reference internal" href="#nagios-agent">Nagios Agent</a></li>
<li><a class="reference internal" href="#nagios-server">Nagios Server</a></li>
<li><a class="reference internal" href="#health-checks">Health Checks</a></li>
</ul>
</li>
<li><a class="reference internal" href="#node-definitions">Node definitions</a></li>
</ul>
</li>
<li><a class="reference internal" href="#deploying-openstack">Deploying OpenStack</a><ul>
<li><a class="reference internal" href="#deploying-via-orchestration">Deploying via orchestration</a></li>
<li><a class="reference internal" href="#installing-openstack-using-puppet-directly">Installing OpenStack using Puppet directly</a></li>
<li><a class="reference internal" href="#examples-of-openstack-installation-sequences">Examples of OpenStack installation sequences</a></li>
</ul>
</li>
<li><a class="reference internal" href="#testing-openstack">Testing OpenStack</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="0045-installation-fuel-web.html"
                        title="previous chapter">Create a multi-node OpenStack cluster using Fuel Web</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="0055-production-considerations.html"
                        title="next chapter">Production Considerations</a></p>
  <h3>This Page</h3>
  <ul class="this-page-menu">
    <li><a href="../_sources/pages/0050-installation-instructions.txt"
           rel="nofollow">Show Source</a></li>
  </ul>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="create-a-multi-node-openstack-cluster-using-fuel">
<span id="create-cluster"></span><h1>Create a multi-node OpenStack cluster using Fuel<a class="headerlink" href="#create-a-multi-node-openstack-cluster-using-fuel" title="Permalink to this headline">¶</a></h1>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><a class="reference internal" href="#how-installation-works" id="id5">How installation works</a></li>
<li><a class="reference internal" href="#before-you-start" id="id6">Before You Start</a></li>
<li><a class="reference internal" href="#infrastructure-allocation-and-installation" id="id7">Infrastructure Allocation and Installation</a><ul>
<li><a class="reference internal" href="#software" id="id8">Software</a></li>
<li><a class="reference internal" href="#network-setup" id="id9">Network setup</a></li>
<li><a class="reference internal" href="#physical-installation-infrastructure" id="id10">Physical installation infrastructure</a></li>
<li><a class="reference internal" href="#virtual-installation-infrastructure" id="id11">Virtual installation infrastructure</a><ul>
<li><a class="reference internal" href="#configuring-virtualbox" id="id12">Configuring VirtualBox</a></li>
<li><a class="reference internal" href="#creating-fuel-pm" id="id13">Creating fuel-pm</a></li>
<li><a class="reference internal" href="#creating-the-openstack-nodes" id="id14">Creating the OpenStack nodes</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#installing-configuring-fuel" id="id15">Installing &amp; Configuring Fuel</a><ul>
<li><a class="reference internal" href="#installing-fuel-from-the-iso" id="id16">Installing Fuel from the ISO</a></li>
<li><a class="reference internal" href="#configuring-fuel-pm-from-the-iso-installation" id="id17">Configuring fuel-pm from the ISO installation</a></li>
</ul>
</li>
<li><a class="reference internal" href="#installing-the-os-using-fuel" id="id18">Installing the OS using Fuel</a><ul>
<li><a class="reference internal" href="#configuring-cobbler-with-config-yaml" id="id19">Configuring Cobbler with config.yaml</a></li>
<li><a class="reference internal" href="#loading-the-configuration" id="id20">Loading the configuration</a></li>
<li><a class="reference internal" href="#installing-the-operating-system" id="id21">Installing the operating system</a></li>
</ul>
</li>
<li><a class="reference internal" href="#generating-the-puppet-manifest" id="id22">Generating the Puppet Manifest</a></li>
<li><a class="reference internal" href="#understanding-the-puppet-manifest" id="id23">Understanding the Puppet Manifest</a><ul>
<li><a class="reference internal" href="#enabling-neutron" id="id24">Enabling Neutron</a></li>
<li><a class="reference internal" href="#defining-the-current-cluster" id="id25">Defining the current cluster</a></li>
<li><a class="reference internal" href="#enabling-cinder" id="id26">Enabling Cinder</a></li>
<li><a class="reference internal" href="#enabling-glance-and-swift" id="id27">Enabling Glance and Swift</a></li>
<li><a class="reference internal" href="#configuring-openstack-to-use-syslog" id="id28">Configuring OpenStack to use syslog</a></li>
<li><a class="reference internal" href="#setting-the-version-and-mirror-type" id="id29">Setting the version and mirror type</a></li>
<li><a class="reference internal" href="#setting-verbosity" id="id30">Setting verbosity</a></li>
<li><a class="reference internal" href="#configuring-rate-limits" id="id31">Configuring Rate-Limits</a></li>
<li><a class="reference internal" href="#enabling-horizon-https-ssl-mode" id="id32">Enabling Horizon HTTPS/SSL mode</a></li>
<li><a class="reference internal" href="#defining-the-node-configurations" id="id33">Defining the node configurations</a></li>
<li><a class="reference internal" href="#installing-nagios-monitoring-using-puppet" id="id34">Installing Nagios Monitoring using Puppet</a><ul>
<li><a class="reference internal" href="#nagios-agent" id="id35">Nagios Agent</a></li>
<li><a class="reference internal" href="#nagios-server" id="id36">Nagios Server</a></li>
<li><a class="reference internal" href="#health-checks" id="id37">Health Checks</a></li>
</ul>
</li>
<li><a class="reference internal" href="#node-definitions" id="id38">Node definitions</a></li>
</ul>
</li>
<li><a class="reference internal" href="#deploying-openstack" id="id39">Deploying OpenStack</a><ul>
<li><a class="reference internal" href="#deploying-via-orchestration" id="id40">Deploying via orchestration</a></li>
<li><a class="reference internal" href="#installing-openstack-using-puppet-directly" id="id41">Installing OpenStack using Puppet directly</a></li>
<li><a class="reference internal" href="#examples-of-openstack-installation-sequences" id="id42">Examples of OpenStack installation sequences</a></li>
</ul>
</li>
<li><a class="reference internal" href="#testing-openstack" id="id43">Testing OpenStack</a></li>
</ul>
</div>
<p>In this section, you’ll learn how to install OpenStack using Fuel and Fuel Web. In addition to getting a feel for the steps involved, you’ll also gain valuable familiarity with some of the customization options. While Fuel provides different deployment configuration templates in the box, it is common for administrators to modify the architecture to meet enterprise requirements. Working hands on with Fuel for OpenStack will help you see how to move certain features around from the standard installation.</p>
<p>The first step, however, is to commit to a deployment template. A balanced, compact, and full-featured deployment is the Multi-node (HA) Compact deployment, so that’s what we’ll be using through the rest of this guide.</p>
<p>Production installations require a physical hardware infrastructure, but you can easily deploy a small simulation cloud on a single physical machine using VirtualBox. You can follow these instructions to install an OpenStack cloud into a test environment using VirtualBox, or to get a production-grade installation using physical hardware.</p>
<div class="section" id="how-installation-works">
<h2><a class="toc-backref" href="#id5">How installation works</a><a class="headerlink" href="#how-installation-works" title="Permalink to this headline">¶</a></h2>
<p>While version 2.0 of Fuel provided the ability to simplify installation of OpenStack, versions 2.1 and above include orchestration capabilities that simplify deployment of OpenStack clusters.  The deployment process follows this general procedure:</p>
<ol class="arabic simple">
<li>Design your architecture.</li>
<li>Install Fuel onto the fuel-pm machine.</li>
<li>Configure Fuel.</li>
<li>Create the basic configuration and load it into Cobbler.</li>
<li>PXE-boot the servers so Cobbler can install the operating system and prepare them for orchestration.</li>
<li>Use one of the templates included in Fuel and the configuration that populates Puppet&#8217;s site.pp file.</li>
<li>Customize the site.pp file as needed.</li>
<li>Use the orchestrator to coordinate the installation of the appropriate OpenStack components on each node.</li>
</ol>
<p>Start by designing your architecture, details about which you can find in the next section, <em class="xref std std-ref">Before You Start</em></p>
</div>
<div class="section" id="before-you-start">
<h2><a class="toc-backref" href="#id6">Before You Start</a><a class="headerlink" href="#before-you-start" title="Permalink to this headline">¶</a></h2>
<p>Before you begin your installation, you will need to make a number of important decisions:</p>
<ul>
<li><p class="first"><strong>OpenStack features.</strong> Your first decision is to decide which of the optional OpenStack features you will need. For example, you must decide whether you want to install Swift, whether you want Glance to use Swift for image storage, whether you want Cinder for block storage, and whether you want nova-network or Neutron (formerly Quantum) to handle your network connectivity. In our example installation we will be installing Glance with Swift support and Cinder for block storage. Also, due to the fact that it can be easily installed using orchestration, we will be using Neutron.</p>
</li>
<li><p class="first"><strong>Deployment configuration.</strong> Next, you need to decide whether your deployment requires high availability (HA). If you need HA for your deployment, you have a choice regarding the number of controllers you want to include. Following the recommendations in the previous section for a typical HA deployment configuration, we will use three OpenStack controllers.</p>
</li>
<li><p class="first"><strong>Cobbler server and Puppet Master.</strong> The heart of any Fuel install is the combination of Puppet Master and Cobbler used to create your resources. Although Cobbler and Puppet Master can be installed on separate machines, it is common practice to install both on a single machine for small to medium size clouds, and that&#8217;s what we&#8217;ll be doing in this example. By default, the Fuel ISO creates a single server with both services.</p>
</li>
<li><p class="first"><strong>Domain name.</strong> Puppet clients generate a Certificate Signing Request (CSR), which is then signed by the Puppet Master. The signed certificate can then be used to authenticate clients during provisioning. Certificate generation requires a fully qualified hostname, so you must choose a domain name to be used in your installation. Future versions of Fuel will enable you to choose this domain name on your own; by default, Fuel 3.1 uses <tt class="docutils literal"><span class="pre">localdomain</span></tt>.</p>
</li>
<li><p class="first"><strong>Network addresses.</strong> OpenStack requires a minimum of three networks. If you are deploying on physical hardware, two of them &#8211; the public network and the internal, or management network &#8211; must be routable in your networking infrastructure. The third network is used by the nodes for inter-node communications. Also, if you intend for your cluster to be accessible from the Internet, you&#8217;ll want the public network to be on the proper network segment.  For simplicity in this case, this example assumes an Internet router at 192.168.0.1.  Additionally, a set of private network addresses should be selected for automatic assignment to guest VMs. (These are fixed IPs for the private network). In our case, we are allocating network addresses as follows:</p>
<blockquote>
<div><ul class="simple">
<li>Public network: 192.168.0.0/24</li>
<li>Internal network: 10.0.0.0/24</li>
<li>Private network: 10.0.1.0/24</li>
</ul>
</div></blockquote>
</li>
<li><p class="first"><strong>Network interfaces.</strong> All of those networks need to be assigned to the available NIC cards on the allocated machines. Additionally, if a fourth NIC is available, Cinder or block storage traffic can be separated and delegated to the fourth NIC. In our case, we&#8217;re assigning networks as follows:</p>
<blockquote>
<div><ul class="simple">
<li>Public network: eth1</li>
<li>Internal network: eth0</li>
<li>Private network: eth2</li>
</ul>
</div></blockquote>
</li>
</ul>
</div>
<div class="section" id="infrastructure-allocation-and-installation">
<h2><a class="toc-backref" href="#id7">Infrastructure Allocation and Installation</a><a class="headerlink" href="#infrastructure-allocation-and-installation" title="Permalink to this headline">¶</a></h2>
<p>The next step is to make sure that you have all of the required hardware and software in place.</p>
<div class="section" id="software">
<h3><a class="toc-backref" href="#id8">Software</a><a class="headerlink" href="#software" title="Permalink to this headline">¶</a></h3>
<p>You can download the latest release of the Fuel ISO from <a class="reference external" href="http://fuel.mirantis.com/your-downloads/">http://fuel.mirantis.com/your-downloads/</a>.</p>
<p>Alternatively, if you can&#8217;t use the pre-built ISO, Mirantis offers the Fuel Library as a tar.gz file downloadable from the <a class="reference external" href="http://fuel.mirantis.com/your-downloads/">Downloads</a> section of the Fuel portal.  Using this file requires a bit more effort, but will yield the same results as using the ISO.</p>
</div>
<div class="section" id="network-setup">
<h3><a class="toc-backref" href="#id9">Network setup</a><a class="headerlink" href="#network-setup" title="Permalink to this headline">¶</a></h3>
<p>OpenStack requires a minimum of three distinct networks: internal (or management), public, and private. The simplest and best methodology to map NICs is to assign each network to a different physical interface. However, not all machines have three NICs, and OpenStack can be configured and deployed with only two physical NICs, combining the internal and public traffic onto a single NIC.</p>
<p>If you are building a simulation environment, you are not limited to the availability of physical NICs. Allocate three NICs to each VM in your OpenStack infrastructure, one each for the internal, public, and private networks.</p>
<p>Finally, we assign network ranges to the internal, public, and private networks, and IP addresses to fuel-pm, fuel-controllers, and fuel-compute nodes. For deployment to a physical infrastructure you must work with your IT department to determine which IPs to use, but for the purposes of this exercise we will assume the below network and IP assignments:</p>
<ol class="arabic simple">
<li>10.0.0.0/24: management or internal network, for communication between Puppet master and Puppet clients, as well as PXE/TFTP/DHCP for Cobbler.</li>
<li>192.168.0.0/24: public network, for the High Availability (HA) Virtual IP (VIP), as well as floating IPs assigned to OpenStack guest VMs</li>
<li>10.0.1.0/24: private network, fixed IPs automatically assigned to guest VMs by OpenStack upon their creation</li>
</ol>
<p>Next we need to allocate a static IP address from the internal network to eth0 for fuel-pm, and eth1 for the controller, compute, and, if necessary, quantum nodes. For High Availability (HA) we must choose and assign an IP address from the public network to HAProxy running on the controllers. You can configure network addresses/network mask according to your needs, but our instructions assume the following network settings on the interfaces:</p>
<ol class="arabic">
<li><p class="first">eth0: internal management network, where each machine is allocated a static IP address from a the defined pool of available addresses:</p>
<blockquote>
<div><ul class="simple">
<li>10.0.0.100 for Puppet Master</li>
<li>10.0.0.101-10.0.0.103 for the controller nodes</li>
<li>10.0.0.110-10.0.0.126 for the compute nodes</li>
<li>10.0.0.10 internal Virtual IP for component access</li>
<li>255.255.255.0 network mask</li>
</ul>
</div></blockquote>
</li>
<li><p class="first">eth1: public network</p>
<blockquote>
<div><ul class="simple">
<li>192.168.0.10 public Virtual IP for access to the Horizon GUI (OpenStack management interface)</li>
</ul>
</div></blockquote>
</li>
<li><p class="first">eth2: for communication between OpenStack VMs without IP address with promiscuous mode enabled.</p>
</li>
</ol>
</div>
<div class="section" id="physical-installation-infrastructure">
<h3><a class="toc-backref" href="#id10">Physical installation infrastructure</a><a class="headerlink" href="#physical-installation-infrastructure" title="Permalink to this headline">¶</a></h3>
<p>The hardware necessary for an installation depends on the choices you have made above. This sample installation requires the following hardware:</p>
<ul>
<li><p class="first">1 server to host both the Puppet Master and Cobbler. The minimum configuration for this server is:</p>
<blockquote>
<div><ul class="simple">
<li>32-bit or 64-bit architecture (64-bit recommended)</li>
<li>1+ CPU or vCPU for up to 10 nodes (2 vCPU for up to 20 nodes, 4 vCPU for up to 100 nodes)</li>
<li>1024+ MB of RAM for up to 10 nodes (4096+ MB for up to 20 nodes, 8192+ MB for up to 100 nodes)</li>
<li>16+ GB of HDD for OS, and Linux distro storage</li>
</ul>
</div></blockquote>
</li>
<li><p class="first">3 servers to act as OpenStack controllers (called fuel-controller-01, fuel-controller-02, and fuel-controller-03 for our sample deployment). The minimum configuration for a controller in Compact mode is:</p>
<blockquote>
<div><ul class="simple">
<li>64-bit architecture</li>
<li>1+ CPU (2 or more CPUs or vCPUs recommended)</li>
<li>1024+ MB of RAM (2048+ MB recommended)</li>
<li>400+ GB of HDD</li>
</ul>
</div></blockquote>
</li>
<li><p class="first">1 server to act as the OpenStack compute node (called fuel-compute-01). The minimum configuration for a compute node with Cinder installed is:</p>
<blockquote>
<div><ul class="simple">
<li>64-bit architecture</li>
<li>2+ CPU, with Intel VTx or AMDV virtualization technology enabled</li>
<li>2048+ MB of RAM</li>
<li>1+ TB of HDD</li>
</ul>
</div></blockquote>
</li>
</ul>
<p>If you choose to deploy Neutron (formerly Quantum) on a separate node, you will need an additional server with specifications comparable to the controller nodes.</p>
<p>Make sure your hardware is capable of PXE booting over the network from Cobbler. You&#8217;ll also need each server&#8217;s mac addresses.</p>
<p>For a list of certified hardware configurations, please <a class="reference external" href="http://www.mirantis.com/contact/">contact the Mirantis Services team</a>.</p>
</div>
<div class="section" id="virtual-installation-infrastructure">
<h3><a class="toc-backref" href="#id11">Virtual installation infrastructure</a><a class="headerlink" href="#virtual-installation-infrastructure" title="Permalink to this headline">¶</a></h3>
<p>For a virtual installation, you need only a single machine. You can get by on 8GB of RAM, but 16GB or more is recommended.</p>
<p>To perform the installation, you need a way to create Virtual Machines. This guide assumes that you are using version 4.2.12 or later of VirtualBox, which you can download from <a class="reference external" href="https://www.virtualbox.org/wiki/Downloads">the VirtualBox site</a>. It is also required that you have the Extension Pack installed to enable features that are needed for a virtualized OpenStack test environment to work correctly.</p>
<p>You&#8217;ll need to run VirtualBox on a stable host system. Mac OS 10.7.x, CentOS 6.3+, or Ubuntu 12.04 are preferred; results in other operating systems are unpredictable. It&#8217;s also important to remember that Windows is incapable of running the installation scripts for Fuel so we cannot recommend Windows as a test platform.</p>
<div class="section" id="configuring-virtualbox">
<h4><a class="toc-backref" href="#id12">Configuring VirtualBox</a><a class="headerlink" href="#configuring-virtualbox" title="Permalink to this headline">¶</a></h4>
<p>If you are on VirtualBox, please create the following host-only adapters and that they are configured correctly:</p>
<ul>
<li><p class="first">VirtualBox -&gt; File -&gt; Preferences...</p>
<blockquote>
<div><ul>
<li><p class="first">Network -&gt; Add HostOnly Adapter (vboxnet0)</p>
<blockquote>
<div><ul class="simple">
<li>IPv4 Address:  10.0.0.1</li>
<li>IPv4 Network Mask:  255.255.255.0</li>
<li>DHCP server: disabled</li>
</ul>
</div></blockquote>
</li>
<li><p class="first">Network -&gt; Add HostOnly Adapter (vboxnet1)</p>
<blockquote>
<div><ul class="simple">
<li>IPv4 Address:  10.0.1.1</li>
<li>IPv4 Network Mask:  255.255.255.0</li>
<li>DHCP server: disabled</li>
</ul>
</div></blockquote>
</li>
<li><p class="first">Network -&gt; Add HostOnly Adapter (vboxnet2)</p>
<blockquote>
<div><ul class="simple">
<li>IPv4 Address:  0.0.0.0</li>
<li>IPv4 Network Mask:  255.255.255.0</li>
<li>DHCP server: disabled</li>
</ul>
</div></blockquote>
</li>
</ul>
</div></blockquote>
</li>
</ul>
<p>In this example, only the first two adapters will be used. If necessasry, though, you can choose to use the third adapter to handle your storage network traffic.</p>
<p>After creating these interfaces, reboot the host machine to make sure that DHCP isn&#8217;t running in the background.</p>
<p>As stated before, installing on Windows isn&#8217;t recommended, but if you&#8217;re attempting to do so you will also need to set up the IP address &amp; network mask under Control Panel &gt; Network and Internet &gt; Network and Sharing Center for the Virtual HostOnly Network adapter.</p>
</div>
<div class="section" id="creating-fuel-pm">
<h4><a class="toc-backref" href="#id13">Creating fuel-pm</a><a class="headerlink" href="#creating-fuel-pm" title="Permalink to this headline">¶</a></h4>
<p>The process of creating a virtual machine to host Fuel in VirtualBox depends on whether your deployment is purely virtual or consists of a physical or virtual fuel-pm controlling physical hardware. If your deployment is purely virtual then Adapter 1 may be a Hostonly adapter attached to vboxnet0, but if your deployment infrastructure consists of a virtual fuel-pm controlling physical machines Adapter 1 must be a Bridged Adapter and connected to whatever network interface of the host machine is connected to your physical machines.</p>
<p>To create fuel-pm, start up VirtualBox and create a new machine as follows:</p>
<ul>
<li><p class="first">Machine -&gt; New...</p>
<blockquote>
<div><ul class="simple">
<li>Name: fuel-pm</li>
<li>Type: Linux</li>
<li>Version: Red Hat (64 Bit)</li>
<li>Memory: 2048 MB</li>
<li>Drive space: 16 GB HDD</li>
</ul>
</div></blockquote>
</li>
<li><p class="first">Machine -&gt; Settings... -&gt; Network</p>
<blockquote>
<div><ul>
<li><p class="first">Adapter 1</p>
<blockquote>
<div><ul>
<li><dl class="first docutils">
<dt>Physical network</dt>
<dd><ul class="first last simple">
<li>Enable Network Adapter</li>
<li>Attached to: Bridged Adapter</li>
<li>Name: The host machine&#8217;s network with access to the network on which the physical machines reside</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>VirtualBox installation</dt>
<dd><ul class="first last simple">
<li>Enable Network Adapter</li>
<li>Attached to: Hostonly Adapter</li>
<li>Name: vboxnet0</li>
</ul>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
</li>
<li><p class="first">Adapter 2</p>
<blockquote>
<div><ul class="simple">
<li>Enable Network Adapter</li>
<li>Attached to: Bridged Adapter</li>
<li>Name: eth0 (or whichever physical network is attached to the Internet)</li>
</ul>
</div></blockquote>
</li>
</ul>
</div></blockquote>
</li>
<li><p class="first">Machine -&gt; Storage</p>
<blockquote>
<div><ul class="simple">
<li>Attach the downloaded ISO as a drive</li>
</ul>
</div></blockquote>
</li>
</ul>
<p>If you cannot or prefer not to install from the ISO, you can find instructions for installing from the Fuel Library in <em class="xref std std-ref">Appendix A</em>.</p>
</div>
<div class="section" id="creating-the-openstack-nodes">
<h4><a class="toc-backref" href="#id14">Creating the OpenStack nodes</a><a class="headerlink" href="#creating-the-openstack-nodes" title="Permalink to this headline">¶</a></h4>
<p>If you&#8217;re using VirtualBox, you will need to create the necessary virtual machines for your OpenStack nodes. Follow these instructions to create machines named fuel-controller-01, fuel-controller-02, fuel- controller-03, and fuel-compute-01. Please, do NOT start these virtual machines until instructed.</p>
<p>As you create each network adapter, click Advanced to expose and record the corresponding mac address.</p>
<ul>
<li><p class="first">Machine -&gt; New...</p>
<blockquote>
<div><ul class="simple">
<li>Name: fuel-controller-01 (you will repeat these steps to create fuel-controller-02, fuel-controller-03, and fuel-compute-01)</li>
<li>Type: Linux</li>
<li>Version: Red Hat (64 Bit)</li>
<li>Memory: 2048MB</li>
<li>Drive space: 8GB</li>
</ul>
</div></blockquote>
</li>
<li><p class="first">Machine -&gt; Settings -&gt; System</p>
<blockquote>
<div><ul class="simple">
<li>Check Network in Boot sequence</li>
</ul>
</div></blockquote>
</li>
<li><p class="first">Machine -&gt; Settings -&gt; Storage</p>
<blockquote>
<div><ul>
<li><p class="first">Controller: SATA</p>
<blockquote>
<div><ul class="simple">
<li>Click the Add icon at the bottom of the Storage Tree pane and choose Add Disk</li>
<li>Add a second VDI disk of 10GB for storage</li>
</ul>
</div></blockquote>
</li>
</ul>
</div></blockquote>
</li>
<li><p class="first">Machine -&gt; Settings -&gt; Network</p>
<blockquote>
<div><ul>
<li><p class="first">Adapter 1</p>
<blockquote>
<div><ul class="simple">
<li>Enable Network Adapter</li>
<li>Attached to: Hostonly Adapter</li>
<li>Name: vboxnet0</li>
</ul>
</div></blockquote>
</li>
<li><p class="first">Adapter 2</p>
<blockquote>
<div><ul class="simple">
<li>Enable Network Adapter</li>
<li>Attached to: Bridged Adapter</li>
<li>Name: eth0 (physical network attached to the Internet. You may also use a gateway if necessary.)</li>
</ul>
</div></blockquote>
</li>
<li><p class="first">Adapter 3</p>
<blockquote>
<div><ul class="simple">
<li>Enable Network Adapter</li>
<li>Attached to: Hostonly Adapter</li>
<li>Name: vboxnet1</li>
<li>Advanced -&gt; Promiscuous mode: Allow All</li>
</ul>
</div></blockquote>
</li>
</ul>
</div></blockquote>
</li>
</ul>
<p>It is important that Adapter 1 is configured to load first as Cobbler will use vboxnet0 for PXE and VirtualBox boots from the LAN using the first available network adapter.</p>
<p>The additional drive volume will be used as storage space by Cinder and will be configured automatically by Fuel.</p>
</div>
</div>
</div>
<div class="section" id="installing-configuring-fuel">
<h2><a class="toc-backref" href="#id15">Installing &amp; Configuring Fuel</a><a class="headerlink" href="#installing-configuring-fuel" title="Permalink to this headline">¶</a></h2>
<p>Having planned your test or production deployment and have determined the resources you will be using, it&#8217;s time to begin putting the pieces together. To do that, you&#8217;ll need to create the Puppet master and Cobbler servers, which will actually provision and configure your OpenStack nodes.</p>
<p>Installing the Puppet Master is a one-time procedure for the entire infrastructure. Once done, Puppet Master will act as a single point of management for all of your servers, and you will never have to return to these installation steps again.</p>
<p>The deployment of the Puppet Master server, named fuel-pm in these instructions, varies slightly between the physical and simulation environments. In a physical infrastructure, fuel-pm must have a network presence on the same network as the physical machines in order to faciliate PXE booting. In a simulation environment fuel-pm only needs host-only virtual network connectivity.</p>
<p>At this point, you should have either a physical or virtual machine that can be booted from the Mirantis ISO, downloaded from <a class="reference external" href="http://fuel.mirantis.com/your-downloads/">http://fuel.mirantis.com/your-downloads/</a>.</p>
<p>This ISO can be used to create fuel-pm on a physical or virtual machine based on CentOS 6.4. If for some reason you cannot use the ISO, follow the instructions in <em class="xref std std-ref">Creating the Puppet master</em> to create your own fuel-pm, then skip ahead to <em class="xref std std-ref">Configuring fuel-pm</em>.</p>
<div class="section" id="installing-fuel-from-the-iso">
<h3><a class="toc-backref" href="#id16">Installing Fuel from the ISO</a><a class="headerlink" href="#installing-fuel-from-the-iso" title="Permalink to this headline">¶</a></h3>
<p>Start the new machine to install the ISO. The only real installation decision you will need to make is to specify the interface through which the installer can access the Internet. Select the eth1 interface you created earlier which should be configured for access to the public network.</p>
</div>
<div class="section" id="configuring-fuel-pm-from-the-iso-installation">
<h3><a class="toc-backref" href="#id17">Configuring fuel-pm from the ISO installation</a><a class="headerlink" href="#configuring-fuel-pm-from-the-iso-installation" title="Permalink to this headline">¶</a></h3>
<p>Once fuel-pm finishes installing, you&#8217;ll be presented with a basic menu. You can use this menu to set the basic information Fuel will need to configure your installation. You can customize these steps to suit your own need, of course, but here are the steps to take for the example installation:</p>
<ol class="arabic simple">
<li>Your admin node must be called <tt class="docutils literal"><span class="pre">fuel-pm</span></tt>, and your domain name must be <tt class="docutils literal"><span class="pre">localdomain</span></tt>.</li>
<li>To configure the management interface, choose 2.<ul>
<li>The example specifies eth0 as the internal, or management interface, so select that interface.</li>
<li>The management network in the example is using static IP addresses, so specify NO for for using DHCP.</li>
<li>Enter the IP address of 10.0.0.100 for the Puppet Master with a netmask of 255.255.255.0.</li>
<li>Set the gateway and DNS servers if desired.  In this example, we&#8217;ll use the router at 192.168.0.1 as the gateway.</li>
</ul>
</li>
<li>To configure the external interface that VMs will use to send traffic to and from the internet, choose 3. Set the interface to eth1. By default, this interface uses DHCP, which is what the example calls for.</li>
<li>To choose the start and end addresses to be used during PXE boot, choose 4.  In the case of this example, the start address is 10.0.0.201 and the end address is 10.0.0.254. Later, these nodes will receive IP addresses from Cobbler.</li>
<li>Future versions of Fuel will enable you to choose a custom set of repositories.</li>
<li>If you need to specify a proxy through which fuel-pm will access the Internet, press 6.</li>
<li>Once you&#8217;ve finished editing, choose 9 to save your changes and exit the menu.</li>
</ol>
<p>Please note:  Even though defaults are shown, you must set actual values; if you simply press &#8220;enter&#8221; you will wind up with empty values.</p>
<p>To re-enter the menu at any time, type:</p>
<p>bootstrap_admin_node.sh</p>
</div>
</div>
<div class="section" id="installing-the-os-using-fuel">
<span id="install-os-using-fuel"></span><h2><a class="toc-backref" href="#id18">Installing the OS using Fuel</a><a class="headerlink" href="#installing-the-os-using-fuel" title="Permalink to this headline">¶</a></h2>
<p>The first step to creating OpenStack nodes is to let Fuel&#8217;s Cobbler kickstart and preseed files assist in the installation of operating systems on the target servers.</p>
<div class="section" id="configuring-cobbler-with-config-yaml">
<span id="configuring-cobbler"></span><h3><a class="toc-backref" href="#id19">Configuring Cobbler with config.yaml</a><a class="headerlink" href="#configuring-cobbler-with-config-yaml" title="Permalink to this headline">¶</a></h3>
<p>Fuel uses the <tt class="docutils literal"><span class="pre">config.yaml</span></tt> file to configure Cobbler and assist in the configuration of the <tt class="docutils literal"><span class="pre">site.pp</span></tt> file.  This file appears in the <tt class="docutils literal"><span class="pre">/root</span></tt> directory when the master node (fuel-pm) is provisioned and configured.</p>
<p>You&#8217;ll want to configure this example to meet your own needs, but the example looks like this:</p>
<div class="highlight-python"><pre>common:
  orchestrator_common:
    attributes:
      deployment_mode: ha_compact
      deployment_engine: simplepuppet
    task_uuid: deployment_task</pre>
</div>
<p>Possible values for <tt class="docutils literal"><span class="pre">deployment_mode</span></tt> are <tt class="docutils literal"><span class="pre">singlenode_compute</span></tt>, <tt class="docutils literal"><span class="pre">multinode_compute</span></tt>, <tt class="docutils literal"><span class="pre">ha_compute</span></tt>, <tt class="docutils literal"><span class="pre">ha_compact</span></tt>, <tt class="docutils literal"><span class="pre">ha_full</span></tt>, and <tt class="docutils literal"><span class="pre">ha_minimal</span></tt>.  For this example, we will set the <tt class="docutils literal"><span class="pre">deployment_mode</span></tt> to <tt class="docutils literal"><span class="pre">ha_compact</span></tt> to tell Fuel to use HA architecture.  Specifying the <tt class="docutils literal"><span class="pre">simplepuppet</span></tt> deployment engine means that the orchestrator will be calling Puppet on each of the nodes.</p>
<p>Next you&#8217;ll need to set OpenStack&#8217;s networking information:</p>
<div class="highlight-python"><pre>openstack_common:
 internal_virtual_ip: 10.0.0.10
 public_virtual_ip: 192.168.0.10
 create_networks: true
 fixed_range: 172.16.0.0/16
 floating_range: 192.168.0.0/24</pre>
</div>
<p>Change the virtual IPs to match the target networks, and set the fixed and floating ranges.</p>
<div class="highlight-python"><pre>swift_loopback: loopback
nv_physical_volumes:
 - /dev/sdb</pre>
</div>
<p>By setting the <tt class="docutils literal"><span class="pre">nv_physical_volumes</span></tt> value, you are not only telling OpenStack to use this value for Cinder (you&#8217;ll see more about that in the <tt class="docutils literal"><span class="pre">site.pp</span></tt> file), but also where Cinder should store its data.</p>
<p>Later, we&#8217;ll set up a new partition for Cinder, so tell Cobbler to create it here.</p>
<div class="highlight-python"><pre>external_ip_info:
  public_net_router: 192.168.0.1
  ext_bridge: 0.0.0.0
  pool_start: 192.168.0.110
  pool_end: 192.168.0.126</pre>
</div>
<p>Set the <tt class="docutils literal"><span class="pre">public_net_router</span></tt> to point to the real router at the public network.  The <tt class="docutils literal"><span class="pre">ext_bridge</span></tt> is the IP of the Neutron (formerly Quantum) bridge. It should assigned to any available free IP on the public network that&#8217;s outside the floating range.  You also have the option to simply set it to <tt class="docutils literal"><span class="pre">0.0.0.0</span></tt>.  The <tt class="docutils literal"><span class="pre">pool_start</span></tt> and <tt class="docutils literal"><span class="pre">pool_end</span></tt> values represent the public addresses of your nodes, and should be within the <tt class="docutils literal"><span class="pre">floating_range</span></tt>.</p>
<div class="highlight-python"><pre>segment_range: 900:999
network_manager: nova.network.manager.FlatDHCPManager
auto_assign_floating_ip: true
quantum_netnode_on_cnt: true</pre>
</div>
<p>Fuel provides two choices for your network manager: FlatDHCPManager, and VlanManager.  By default, the system uses FlatDHCPManager.  Here you can see that we&#8217;re also telling OpenStack to automatically assing a floating IP to an instance when it&#8217;s created, and to put the Neutron services on the controllers rather than a sepearate node.You can also choose <tt class="docutils literal"><span class="pre">tenant_network_type</span></tt> for network segmentation type and  segmentation range <tt class="docutils literal"><span class="pre">segment_range</span></tt>  for network (consult Neutron documentation for details).</p>
<div class="highlight-python"><pre>use_syslog: false
syslog_server: 127.0.0.1
mirror_type: default</pre>
</div>
<p><strong>THIS SETTING IS CRUCIAL:</strong> The <tt class="docutils literal"><span class="pre">mirror_type</span></tt> <strong>must</strong> to be set to <tt class="docutils literal"><span class="pre">default</span></tt> unless you have your own repositories set up, or OpenStack will not install properly.</p>
<div class="highlight-python"><pre>quantum: true
internal_interface: eth0
public_interface: eth1
private_interface: eth2
public_netmask: 255.255.255.0
internal_netmask: 255.255.255.0</pre>
</div>
<p>Earlier, you decided which interfaces to use for which networks; note that here.</p>
<div class="highlight-python"><pre>default_gateway: 192.168.0.1</pre>
</div>
<p>Depending on how you&#8217;ve set up your network, you can either set the <tt class="docutils literal"><span class="pre">default_gateway</span></tt> to the master node (fuel-pm) or to the <tt class="docutils literal"><span class="pre">public_net_router</span></tt>.</p>
<div class="highlight-python"><pre>nagios_master: fuel-controller-01.localdomain
loopback: loopback
cinder: true
cinder_nodes:
- controller
swift: true</pre>
</div>
<p>The loopback setting determines how Swift stores data. If you set the value to <tt class="docutils literal"><span class="pre">loopback</span></tt>, Swift will use 1gb files as storage devices. If you tuned Cobbler to create a partition for Swift and mounted it to <tt class="docutils literal"><span class="pre">/srv/nodes/</span></tt>, then you should set <tt class="docutils literal"><span class="pre">loopback</span></tt> to <tt class="docutils literal"><span class="pre">false</span></tt>.</p>
<p>In this example, you&#8217;re using Cinder and including it on the compute nodes, so note that appropriately.  Also, you&#8217;re using Swift, so turn that on here.</p>
<div class="highlight-python"><pre>repo_proxy: http://10.0.0.100:3128</pre>
</div>
<p>One improvement in Fuel 2.1 was the ability for the master node to cache downloads in order to speed up installs; by default the <tt class="docutils literal"><span class="pre">repo_proxy</span></tt> is set to point to fuel-pm in order to let that happen.  One consequence of that is that your deployment will actually go faster if you let one install complete, then do all the others, rather than running all of them concurrently.</p>
<div class="highlight-python"><pre>deployment_id: '53'</pre>
</div>
<p>Fuel enables you to manage multiple clusters; setting the <tt class="docutils literal"><span class="pre">deployment_id</span></tt> will let Fuel know which deployment you&#8217;re working with.</p>
<div class="highlight-python"><pre>dns_nameservers:
- 10.0.0.100
- 8.8.8.8</pre>
</div>
<p>The slave nodes should first look to the master node for DNS, so mark that as your first nameserver.</p>
<p>The next step is to define the nodes themselves.  To do that, you&#8217;ll list each node once for each role that needs to be installed.  Note that by default the first node is called <tt class="docutils literal"><span class="pre">fuel-cobbler</span></tt>; change it to <tt class="docutils literal"><span class="pre">fuel-pm</span></tt>.</p>
<div class="highlight-python"><pre>nodes:
- name: fuel-pm
  role: cobbler
  internal_address: 10.0.0.100
  public_address: 192.168.0.100
- name: fuel-controller-01
  role: controller
  internal_address: 10.0.0.101
  public_address: 192.168.0.101
  swift_zone: 1
- name: fuel-controller-02
  role: controller
  internal_address: 10.0.0.102
  public_address: 192.168.0.102
  swift_zone: 2
- name: fuel-controller-03
  role: controller
  internal_address: 10.0.0.103
  public_address: 192.168.0.103
  swift_zone: 3
- name: fuel-controller-01
  role: quantum
  internal_address: 10.0.0.101
  public_address: 192.168.0.101
- name: fuel-compute-01
  role: compute
  internal_address: 10.0.0.110
  public_address: 192.168.0.110</pre>
</div>
<p>Notice that each node can be listed multiple times; this is because each node fulfills multiple roles.  Notice also that the IP address for fuel-compute-01 is <a href="#id1"><span class="problematic" id="id2">*</span></a>.110, not <a href="#id3"><span class="problematic" id="id4">*</span></a>.105.</p>
<p>The <tt class="docutils literal"><span class="pre">cobbler_common</span></tt> section applies to all machines:</p>
<div class="highlight-python"><pre>cobbler_common:
  # for Centos
  profile: "centos64_x86_64"
  # for Ubuntu
  # profile: "ubuntu_1204_x86_64"</pre>
</div>
<p>Fuel can install CentOS or Ubuntu on your servers, or you can add a profile of your own. By default, <tt class="docutils literal"><span class="pre">config.yaml</span></tt> uses CentOS.</p>
<div class="highlight-python"><pre>netboot-enabled: "1"
# for Ubuntu
# ksmeta: "puppet_version=2.7.19-1puppetlabs2 \
# for Centos
name-servers: "10.0.0.100"
name-servers-search: "localdomain"
gateway: 192.168.0.1</pre>
</div>
<p>Set the default nameserver to be fuel-pm, and change the domain name to your own domain name.  Set the <tt class="docutils literal"><span class="pre">gateway</span></tt> to the public network&#8217;s default gateway. Alternatively, if you don&#8217;t plan to use your public networks actual gateway, you can set this value to be the IP address of the master node.</p>
<p><strong>Please note:</strong> You must specify a working gateway (or proxy) in order to install OpenStack, because the system will need to communicate with public repositories.</p>
<div class="highlight-python"><pre>ksmeta: "puppet_version=2.7.19-1puppetlabs2 \
  puppet_auto_setup=1 \
  puppet_master=fuel-pm.localdomain \</pre>
</div>
<p>Change the fully-qualified domain name for the Puppet Master to reflect your own domain name.</p>
<div class="highlight-python"><pre>puppet_enable=0 \
ntp_enable=1 \
mco_auto_setup=1 \
mco_pskey=un0aez2ei9eiGaequaey4loocohjuch4Ievu3shaeweeg5Uthi \
mco_stomphost=10.0.0.100 \</pre>
</div>
<p>Make sure the <tt class="docutils literal"><span class="pre">mco_stomphost</span></tt> is set for the master node so that the orchestrator can find the nodes.</p>
<div class="highlight-python"><pre>mco_stompport=61613 \
mco_stompuser=mcollective \
mco_stomppassword=AeN5mi5thahz2Aiveexo \
mco_enable=1"</pre>
</div>
<p>This section sets the system up for orchestration; you shouldn&#8217;t have to touch it.</p>
<p>Next you&#8217;ll define the actual servers.</p>
<div class="highlight-python"><pre>fuel-controller-01:
  hostname: "fuel-controller-01"
  role: controller
  interfaces:
    eth0:
      mac: "08:00:27:BD:3A:7D"
      static: "1"
      ip-address: "10.0.0.101"
      netmask: "255.255.255.0"
      dns-name: "fuel-controller-01.localdomain"
      management: "1"
    eth1:
      mac: "08:00:27:ED:9C:3C"
      static: "0"
    eth2:
      mac: "08:00:27:B0:EB:2C"
      static: "1"
  interfaces_extra:
    eth0:
      peerdns: "no"
    eth1:
      peerdns: "no"
    eth2:
      promisc: "yes"
      userctl: "yes"
      peerdns: "no"</pre>
</div>
<p>For a VirtualBox installation, you can retrieve the MAC ids for your network adapters by expanding &#8220;Advanced&#8221; for the adapater in VirtualBox, or by executing ifconfig on the server itself.</p>
<p>For a physical installation, the MAC address of the server is often printed on the sticker attached to the server for the LOM interfaces, or is available from the BIOS screen.  You may also be able to find the MAC address in the hardware inventory BMC/DRAC/ILO, though this may be server-dependent.</p>
<p>Also, make sure the <tt class="docutils literal"><span class="pre">ip-address</span></tt> is correct, and that the <tt class="docutils literal"><span class="pre">dns-name</span></tt> has your own domain name in it.</p>
<p>In this example, IP addresses should be assigned as follows:</p>
<div class="highlight-python"><pre>fuel-controller-01:  10.0.0.101
fuel-controller-02:  10.0.0.102
fuel-controller-03:  10.0.0.103
fuel-compute-01:     10.0.0.110</pre>
</div>
<p>Repeat this step for each of the other controllers, and for the compute node.  Note that the compute node has its own role:</p>
<div class="highlight-python"><pre>fuel-compute-01:
  hostname: "fuel-compute-01"
  role: compute
  interfaces:
    eth0:
      mac: "08:00:27:AE:A9:6E"
      static: "1"
      ip-address: "10.0.0.110"
      netmask: "255.255.255.0"
      dns-name: "fuel-compute-01.localdomain"
      management: "1"
    eth1:
      mac: "08:00:27:B7:F9:CD"
      static: "0"
    eth2:
      mac: "08:00:27:8B:A6:B7"
      static: "1"
  interfaces_extra:
    eth0:
      peerdns: "no"
    eth1:
      peerdns: "no"
    eth2:
      promisc: "yes"
      userctl: "yes"
      peerdns: "no"</pre>
</div>
</div>
<div class="section" id="loading-the-configuration">
<h3><a class="toc-backref" href="#id20">Loading the configuration</a><a class="headerlink" href="#loading-the-configuration" title="Permalink to this headline">¶</a></h3>
<p>Once you&#8217;ve completed the changes to <tt class="docutils literal"><span class="pre">config.yaml</span></tt>, you need to load the information into Cobbler.  To do that, use the <tt class="docutils literal"><span class="pre">cobbler_system</span></tt> script:</p>
<div class="highlight-python"><pre>cobbler_system -f config.yaml</pre>
</div>
<p>Now you&#8217;re ready to start spinning up the controllers and compute nodes.</p>
</div>
<div class="section" id="installing-the-operating-system">
<h3><a class="toc-backref" href="#id21">Installing the operating system</a><a class="headerlink" href="#installing-the-operating-system" title="Permalink to this headline">¶</a></h3>
<p>Now that Cobbler has the correct configuration, the only thing you
need to do is to PXE-boot your nodes. This means that they will boot over the network, with
DHCP/TFTP provided by Cobbler, and will be provisioned accordingly,
with the specified operating system and configuration.</p>
<p>If you installed Fuel from the ISO, start fuel-controller-01 first and let the installation finish before starting the other nodes; Fuel will cache the downloads so subsequent installs will go faster.</p>
<p>The process for each node looks like this:</p>
<ol class="arabic">
<li><p class="first">Start the VM.</p>
</li>
<li><p class="first">Press F12 immediately and select l (LAN) as a bootable media.</p>
</li>
<li><p class="first">Wait for the installation to complete.</p>
</li>
<li><p class="first">Log into the new machine using root/r00tme.</p>
</li>
<li><p class="first"><strong>Change the root password.</strong></p>
</li>
<li><p class="first">Check that networking is set up correctly and the machine can reach the Internet:</p>
<div class="highlight-python"><pre>ping fuel-pm.localdomain
ping www.mirantis.com</pre>
</div>
</li>
</ol>
<p>If you&#8217;re unable to ping outside addresses, add the fuel-pm server as a default gateway:</p>
<div class="highlight-python"><pre>route add default gw 10.0.0.100</pre>
</div>
<p><strong>It is important to note</strong> that if you use VLANs in your network
configuration, you always have to keep in mind the fact that PXE
booting does not work on tagged interfaces. Therefore, all your nodes,
including the one where the Cobbler service resides, must share one
untagged VLAN (also called native VLAN). If necessary, you can use the
<tt class="docutils literal"><span class="pre">dhcp_interface</span></tt> parameter of the <tt class="docutils literal"><span class="pre">cobbler::server</span></tt> class to bind the DHCP
service to the appropriate interface.</p>
</div>
</div>
<div class="section" id="generating-the-puppet-manifest">
<h2><a class="toc-backref" href="#id22">Generating the Puppet Manifest</a><a class="headerlink" href="#generating-the-puppet-manifest" title="Permalink to this headline">¶</a></h2>
<p>Before you can deploy OpenStack, you will need to configure the site.pp file. Previous versions of Fuel required you to manually configure <tt class="docutils literal"><span class="pre">site.pp</span></tt>. Version 3.1 includes the <tt class="docutils literal"><span class="pre">openstack_system</span></tt> script, which uses the <tt class="docutils literal"><span class="pre">config.yaml</span></tt> file and reference architecture templates to create the appropriate Puppet manifest.  To create <tt class="docutils literal"><span class="pre">site.pp</span></tt>, execute this command:</p>
<div class="highlight-python"><pre>openstack_system -c config.yaml \
  -t /etc/puppet/modules/openstack/examples/site_openstack_ha_compact.pp \
  -o /etc/puppet/manifests/site.pp \
  -a astute.yaml</pre>
</div>
<p>The four parameters in the command above are:</p>
<blockquote>
<div><ul class="simple">
<li><tt class="docutils literal"><span class="pre">-c</span></tt>:  The absolute or relative path to the <tt class="docutils literal"><span class="pre">config.yaml</span></tt> file you customized earlier.</li>
<li><tt class="docutils literal"><span class="pre">-t</span></tt>:  The template file to serve as a basis for <tt class="docutils literal"><span class="pre">site.pp</span></tt>.  Possible templates include <tt class="docutils literal"><span class="pre">site_openstack_ha_compact.pp</span></tt>, <tt class="docutils literal"><span class="pre">site_openstack_ha_minimal.pp</span></tt>, <tt class="docutils literal"><span class="pre">site_openstack_ha_full.pp</span></tt>, <tt class="docutils literal"><span class="pre">site_openstack_single.pp</span></tt>, and <tt class="docutils literal"><span class="pre">site_openstack_simple.pp</span></tt>.</li>
<li><tt class="docutils literal"><span class="pre">-o</span></tt>:  The output file.  This should always be <tt class="docutils literal"><span class="pre">/etc/puppet/manifests/site.pp</span></tt>.</li>
<li><tt class="docutils literal"><span class="pre">-a</span></tt>:  The orchestration configuration file, to be output for use in the next step.</li>
</ul>
</div></blockquote>
<p>From there you&#8217;re ready to install your OpenStack components. Before that, however, let&#8217;s look at what is actually in the new <tt class="docutils literal"><span class="pre">site.pp</span></tt> manifest, so that you can understand how to customize it if necessary.  (Similarly, if you are installing Fuel Library without the ISO, you will need to make these customizations yourself.)</p>
</div>
<div class="section" id="understanding-the-puppet-manifest">
<h2><a class="toc-backref" href="#id23">Understanding the Puppet Manifest</a><a class="headerlink" href="#understanding-the-puppet-manifest" title="Permalink to this headline">¶</a></h2>
<p>At this point you should have functioning servers that are ready to take an OpenStack installation. If you&#8217;re using VirtualBox, save the current state of every virtual machine by taking a snapshot using <tt class="docutils literal"><span class="pre">File-&gt;Take</span> <span class="pre">Snapshot</span></tt>. Snapshots are a useful tool when you make a mistake, encounter an issue, or just want to try different configurations, all without having to start from scratch.</p>
<p>Next, go through the <tt class="docutils literal"><span class="pre">/etc/puppet/manifests/site.pp</span></tt> file and make any necessary customizations.  If you have run <tt class="docutils literal"><span class="pre">openstack_system</span></tt>, there shouldn&#8217;t be anything to change (with one small exception) but if you are installing Fuel manually, you will need to make these changes yourself.</p>
<p>Let&#8217;s start with the basic network customization:</p>
<div class="highlight-python"><pre>### GENERAL CONFIG ###
# This section sets main parameters such as hostnames and IP addresses of different nodes

# This is the name of the public interface. The public network provides address space for Floating IPs, as well as public IP accessibility to the API endpoints.
$public_interface = 'eth1'
$public_br = 'br-ex'

# This is the name of the internal interface. It will be attached to the management network, where data exchange between components of the OpenStack cluster will happen.
$internal_interface = 'eth0'
$internal_br = 'br-mgmt'

# This is the name of the private interface. All traffic within OpenStack tenants' networks will go through this interface.
$private_interface = 'eth2'</pre>
</div>
<p>In this case, we don&#8217;t need to make any changes to the interface settings, because they match what we&#8217;ve already set up.</p>
<div class="highlight-python"><pre># Public and Internal VIPs. These virtual addresses are required by HA topology and will be managed by keepalived.
$internal_virtual_ip = '10.0.0.10'

# Change this IP to IP routable from your 'public' network,
# e. g. Internet or your office LAN, in which your public
# interface resides
$public_virtual_ip = '192.168.0.10'</pre>
</div>
<p>Make sure the virtual IPs you see here don&#8217;t conflict with your network configuration. The IPs you use should be routeable, but not within the range of a DHCP scope.   These are the IPs through which your services will be accessed.</p>
<p>The following section configures the servers themselves.  If you are setting up Fuel manually, make sure to add each server with the appropriate IP addresses; if you ran <tt class="docutils literal"><span class="pre">openstack_system</span></tt>, the values will be overridden by the next section, and you can ignore this array.</p>
<div class="highlight-python"><pre>$nodes_harr = [
  {
    'name' =&gt; 'fuel-pm',
    'role' =&gt; 'cobbler',
    'internal_address' =&gt; '10.0.0.100',
    'public_address'   =&gt; '192.168.0.100',
    'mountpoints'=&gt; "1 1\n2 1",
    'storage_local_net_ip' =&gt; '10.0.0.100',
  },
  {
    'name' =&gt; 'fuel-controller-01',
    'role' =&gt; 'primary-controller',
    'internal_address' =&gt; '10.0.0.101',
    'public_address'   =&gt; '192.168.0.101',
    'mountpoints'=&gt; "1 1\n2 1",
    'storage_local_net_ip' =&gt; '10.0.0.101',
  },
  {
    'name' =&gt; 'fuel-controller-02',
    'role' =&gt; 'controller',
    'internal_address' =&gt; '10.0.0.102',
    'public_address'   =&gt; '192.168.0.102',
    'mountpoints'=&gt; "1 1\n2 1",
    'storage_local_net_ip' =&gt; '10.0.0.102',
  },
  {
    'name' =&gt; 'fuel-controller-03',
    'role' =&gt; 'controller',
    'internal_address' =&gt; '10.0.0.105',
    'public_address'   =&gt; '192.168.0.105',
    'mountpoints'=&gt; "1 1\n2 1",
    'storage_local_net_ip' =&gt; '10.0.0.105',
  },
  {
    'name' =&gt; 'fuel-compute-01',
    'role' =&gt; 'compute',
    'internal_address' =&gt; '10.0.0.106',
    'public_address'   =&gt; '192.168.0.106',
    'mountpoints'=&gt; "1 1\n2 1",
    'storage_local_net_ip' =&gt; '10.0.0.106',
  }
]</pre>
</div>
<p>Because this section comes from a template, it will likely include a number of servers you&#8217;re not using; feel free to leave them or take them out.</p>
<p>Next, the <tt class="docutils literal"><span class="pre">site.pp</span></tt> file lists all of the nodes and roles you defined in the <tt class="docutils literal"><span class="pre">config.yaml</span></tt> file:</p>
<div class="highlight-python"><pre>$nodes = [{'public_address' =&gt; '192.168.0.101','name' =&gt; 'fuel-controller-01','role' =&gt;
           'primary-controller','internal_address' =&gt; '10.0.0.101',
           'storage_local_net_ip' =&gt; '10.0.0.101', 'mountpoints' =&gt; '1 2\n2 1',
           'swift-zone' =&gt; 1 },
          {'public_address' =&gt; '192.168.0.102','name' =&gt; 'fuel-controller-02','role' =&gt;
           'controller','internal_address' =&gt; '10.0.0.102',
           'storage_local_net_ip' =&gt; '10.0.0.102', 'mountpoints' =&gt; '1 2\n2 1',
           'swift-zone' =&gt; 2},
          {'public_address' =&gt; '192.168.0.103','name' =&gt; 'fuel-controller-03','role' =&gt;
           'storage','internal_address' =&gt; '10.0.0.103',
           'storage_local_net_ip' =&gt; '10.0.0.103', 'mountpoints' =&gt; '1 2\n2 1',
           'swift-zone' =&gt; 3},
          {'public_address' =&gt; '192.168.0.110','name' =&gt; 'fuel-compute-01','role' =&gt;
           'compute','internal_address' =&gt; '10.0.0.110'}]</pre>
</div>
<p>Possible roles include ‘compute’,  ‘controller’, ‘primary-controller’, ‘storage’, ‘swift-proxy’, ‘quantum’, ‘master’, and ‘cobbler’. Check the IP addresses for each node and make sure that they match the contents of this array.</p>
<p>The file also specifies the default gateway to be the fuel-pm machine:</p>
<div class="highlight-python"><pre>$default_gateway = '192.168.0.1'</pre>
</div>
<p>Next <tt class="docutils literal"><span class="pre">site.pp</span></tt> defines DNS servers and provides netmasks:</p>
<div class="highlight-python"><pre># Specify nameservers here.
# You can point this to the cobbler node IP, or to specially prepared nameservers as needed.
$dns_nameservers = ['10.0.0.100','8.8.8.8']

# Specify netmasks for internal and external networks.
$internal_netmask = '255.255.255.0'
$public_netmask = '255.255.255.0'
...
# Set this to anything other than pacemaker if you do not want Neutron HA (formerly Quantum HA)
# Also, if you do not want Neutron HA, you MUST enable $quantum_network_node
# only on the controller
$ha_provider = 'pacemaker'
$use_unicast_corosync = false</pre>
</div>
<p>Next specify the main controller as the Nagios master.</p>
<div class="highlight-python"><pre># Set nagios master fqdn
$nagios_master = 'fuel-controller-01.localdomain'
## proj_name  name of environment nagios configuration
$proj_name            = 'test'</pre>
</div>
<p>Here again we have a parameter that looks ahead to things to come; OpenStack supports monitoring via Nagios.  In this section, you can choose the Nagios master server as well as setting a project name.</p>
<div class="highlight-python"><pre>#Specify if your installation contains multiple Nova controllers. Defaults to true as it is the most common scenario.
$multi_host              = true</pre>
</div>
<p>A single host cloud isn&#8217;t especially useful, but if you really want to, you can specify that here.</p>
<p>Finally, you can define the various usernames and passwords for OpenStack services.</p>
<div class="highlight-python"><pre># Specify different DB credentials for various services
$mysql_root_password     = 'nova'
$admin_email             = 'openstack@openstack.org'
$admin_password          = 'nova'

$keystone_db_password    = 'nova'
$keystone_admin_token    = 'nova'

$glance_db_password      = 'nova'
$glance_user_password    = 'nova'

$nova_db_password        = 'nova'
$nova_user_password      = 'nova'

$rabbit_password         = 'nova'
$rabbit_user             = 'nova'

$swift_user_password     = 'swift_pass'
$swift_shared_secret     = 'changeme'

$quantum_user_password   = 'quantum_pass'
$quantum_db_password     = 'quantum_pass'
$quantum_db_user         = 'quantum'
$quantum_db_dbname       = 'quantum'

# End DB credentials section</pre>
</div>
<p>Now that the network is configured for the servers, let&#8217;s look at the various OpenStack services.</p>
<div class="section" id="enabling-neutron">
<h3><a class="toc-backref" href="#id24">Enabling Neutron</a><a class="headerlink" href="#enabling-neutron" title="Permalink to this headline">¶</a></h3>
<p>In order to deploy OpenStack with Neutron you need to set up an additional node that will act as an L3 router, or run Neutron out of one of the existing nodes.</p>
<div class="highlight-python"><pre>### NETWORK/QUANTUM ###
# Specify network/quantum specific settings

# Should we use quantum or nova-network (deprecated).
# Consult OpenStack documentation for differences between them.
$quantum = true
$quantum_netnode_on_cnt  = true</pre>
</div>
<p>In this case, we&#8217;re using a &#8220;compact&#8221; architecture, so we want to install Neutron on the controllers:</p>
<div class="highlight-python"><pre># Specify network creation criteria:
# Should puppet automatically create networks?
$create_networks = true

# Fixed IP addresses are typically used for communication between VM instances.
$fixed_range = '172.16.0.0/16'

# Floating IP addresses are used for communication of VM instances with the outside world (e.g. Internet).
$floating_range = '192.168.0.0/24'</pre>
</div>
<p>OpenStack uses two ranges of IP addresses for virtual machines: fixed IPs, which are used for communication between VMs, and thus are part of the private network, and floating IPs, which are assigned to VMs for the purpose of communicating to and from the Internet.</p>
<div class="highlight-python"><pre># These parameters are passed to the previously specified network manager , e.g. nova-manage network create.
# Not used in Neutron.
$num_networks    = 1
$network_size    = 31
$vlan_start      = 300</pre>
</div>
<p>These values don&#8217;t actually relate to Neutron; they are used by nova-network.  IDs for the VLANs OpenStack will create for tenants run from <tt class="docutils literal"><span class="pre">vlan_start</span></tt> to (<tt class="docutils literal"><span class="pre">vlan_start</span> <span class="pre">+</span> <span class="pre">num_networks</span> <span class="pre">-</span> <span class="pre">1</span></tt>), and are generated automatically.</p>
<div class="highlight-python"><pre># Neutron

# Segmentation type for isolating traffic between tenants
# Consult Openstack Neutron docs
$tenant_network_type     = 'gre'

# Which IP address will be used for creating GRE tunnels.
$quantum_gre_bind_addr = $internal_address</pre>
</div>
<p>If you are installing Neutron in non-HA mode, you will need to specify which single controller controls Neutron.</p>
<div class="highlight-python"><pre># If $external_ipinfo option is not defined, the addresses will be allocated automatically from $floating_range:
# the first address will be defined as an external default router,
# the second address will be attached to an uplink bridge interface,
# the remaining addresses will be utilized for the floating IP address pool.
$external_ipinfo = {
   'pool_start' =&gt; '192.168.0.115',
       'public_net_router' =&gt; '192.168.0.1',
       'pool_end' =&gt; '192.168.0.126',
       'ext_bridge' =&gt; '0.0.0.0'
}

# Neutron segmentation range.
# For VLAN networks: valid VLAN VIDs can be 1 through 4094.
# For GRE networks: Valid tunnel IDs can be any 32-bit unsigned integer.
$segment_range = '900:999'

# Set up OpenStack network manager. It is used ONLY in nova-network.
# Consult Openstack nova-network docs for possible values.
$network_manager = 'nova.network.manager.FlatDHCPManager'

# Assign floating IPs to VMs on startup automatically?
$auto_assign_floating_ip = false

# Database connection for Neutron configuration (quantum.conf)
$quantum_sql_connection  = "mysql://${quantum_db_user}:${quantum_db_password}@${$internal_virtual_ip}/{quantum_db_dbname}"

if $quantum {
  $public_int   = $public_br
  $internal_int = $internal_br
} else {
  $public_int   = $public_interface
  $internal_int = $internal_interface
}</pre>
</div>
<p>If the system is set up to use Neutron, the public and internal interfaces are set to use the appropriate bridges, rather than the defined interfaces.</p>
<p>The remaining configuration is used to define classes that will be added to each Neutron node:</p>
<div class="highlight-python"><pre>#Network configuration
stage {'netconfig':
      before  =&gt; Stage['main'],
}
class {'l23network': use_ovs =&gt; $quantum, stage=&gt; 'netconfig'}
class node_netconfig (
  $mgmt_ipaddr,
  $mgmt_netmask  = '255.255.255.0',
  $public_ipaddr = undef,
  $public_netmask= '255.255.255.0',
  $save_default_gateway=true,
  $quantum = $quantum,
) {
  if $quantum {
    l23network::l3::create_br_iface {'mgmt':
      interface =&gt; $internal_interface, # !!! NO $internal_int /sv !!!
      bridge    =&gt; $internal_br,
      ipaddr    =&gt; $mgmt_ipaddr,
      netmask   =&gt; $mgmt_netmask,
      dns_nameservers      =&gt; $dns_nameservers,
      save_default_gateway =&gt; $save_default_gateway,
    } -&gt;
    l23network::l3::create_br_iface {'ex':
      interface =&gt; $public_interface, # !! NO $public_int /sv !!!
      bridge    =&gt; $public_br,
      ipaddr    =&gt; $public_ipaddr,
      netmask   =&gt; $public_netmask,
      gateway   =&gt; $default_gateway,
    }
  } else {
    # nova-network mode
    l23network::l3::ifconfig {$public_int:
      ipaddr  =&gt; $public_ipaddr,
      netmask =&gt; $public_netmask,
      gateway =&gt; $default_gateway,
    }
    l23network::l3::ifconfig {$internal_int:
      ipaddr  =&gt; $mgmt_ipaddr,
      netmask =&gt; $mgmt_netmask,
      dns_nameservers      =&gt; $dns_nameservers,
    }
  }
  l23network::l3::ifconfig {$private_interface: ipaddr=&gt;'none' }
}
### NETWORK/QUANTUM END ###</pre>
</div>
<p>All of this assumes, of course, that you&#8217;re using Neutron; if you&#8217;re using nova-network instead, only these values apply.</p>
</div>
<div class="section" id="defining-the-current-cluster">
<h3><a class="toc-backref" href="#id25">Defining the current cluster</a><a class="headerlink" href="#defining-the-current-cluster" title="Permalink to this headline">¶</a></h3>
<p>Fuel enables you to control multiple deployments simultaneously by setting an individual deployment ID:</p>
<div class="highlight-python"><pre># This parameter specifies the the identifier of the current cluster. This is required for environments where you have multiple deployments.
# installation. Each cluster requires a unique integer value.
# Valid identifier range is 0 to 254
$deployment_id = '79'</pre>
</div>
</div>
<div class="section" id="enabling-cinder">
<h3><a class="toc-backref" href="#id26">Enabling Cinder</a><a class="headerlink" href="#enabling-cinder" title="Permalink to this headline">¶</a></h3>
<p>Our example uses Cinder, and with some very specific variations from the default. Specifically, as we said before, while the Cinder scheduler will continue to run on the controllers, the actual storage takes place on the compute nodes, specifically the <tt class="docutils literal"><span class="pre">/dev/sdb1</span></tt> partition you created earlier. Cinder will be activated on any node that contains the specified block devices &#8211; unless specified otherwise &#8211; so let&#8217;s look at what all of that means for the configuration.</p>
<div class="highlight-python"><pre># Choose which nodes to install cinder onto
# 'compute'            -&gt; compute nodes will run cinder
# 'controller'         -&gt; controller nodes will run cinder
# 'storage'            -&gt; storage nodes will run cinder
# 'fuel-controller-XX' -&gt; specify particular host(s) by hostname
# 'XXX.XXX.XXX.XXX'    -&gt; specify particular host(s) by IP address
# 'all'                -&gt; compute, controller, and storage nodes will run cinder (excluding swift and proxy nodes)
$cinder_nodes          = ['controller']</pre>
</div>
<p>We want Cinder to be on the controller nodes, so set this value to <tt class="docutils literal"><span class="pre">['controller']</span></tt>.</p>
<div class="highlight-python"><pre># Set this option to true if cinder-volume has been installed to the host
# otherwise it will install api and scheduler services
$manage_volumes = true

# Setup network interface, which Cinder uses to export iSCSI targets.
$cinder_iscsi_bind_addr = $internal_address</pre>
</div>
<p>Here you have the opportunity to specify which network interface Cinder uses for its own traffic. For example, you could set up a fourth NIC at <tt class="docutils literal"><span class="pre">eth3</span></tt> and specify that rather than <tt class="docutils literal"><span class="pre">$internal_int</span></tt>.</p>
<div class="highlight-python"><pre># Below you can add physical volumes to cinder. Please replace values with the actual names of devices.
# This parameter defines which partitions to aggregate into cinder-volumes or nova-volumes LVM VG
# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
# USE EXTREME CAUTION WITH THIS SETTING! IF THIS PARAMETER IS DEFINED,
# IT WILL AGGREGATE THE VOLUMES INTO AN LVM VOLUME GROUP
# AND ALL THE DATA THAT RESIDES ON THESE VOLUMES WILL BE LOST!
# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
# Leave this parameter empty if you want to create [cinder|nova]-volumes VG by yourself
$nv_physical_volume = ['/dev/sdb']

#Evaluate cinder node selection
if ($cinder) {
  if (member($cinder_nodes,'all')) {
     $is_cinder_node = true
  } elsif (member($cinder_nodes,$::hostname)) {
     $is_cinder_node = true
  } elsif (member($cinder_nodes,$internal_address)) {
     $is_cinder_node = true
  } elsif ($node[0]['role'] =~ /controller/)) {
     $is_cinder_node = member($cinder_nodes, 'controller')
  } else {
     $is_cinder_node = member($cinder_nodes, $node[0]['role'])
  }
} else {
  $is_cinder_node = false
}

### CINDER/VOLUME END ###</pre>
</div>
<p>We only want to allocate the <tt class="docutils literal"><span class="pre">/dev/sdb</span></tt> volume to Cinder, so adjust <tt class="docutils literal"><span class="pre">$nv_physical_volume</span></tt> accordingly. Note, however, that this is a global value; it will apply to all servers, including the controllers &#8211; unless we specify otherwise, which we illustrate below.</p>
<p><strong>Be careful</strong> to not add block devices to the list which contain useful data (e.g. block devices on which your OS resides), as they will be destroyed after you allocate them for Cinder. It is always a good rule of thumb to deploy OpenStack on blank storage and move content to those volumes later instead of try to retain existing data.</p>
<p>Now lets look at Swift, the other storage-based service option.</p>
</div>
<div class="section" id="enabling-glance-and-swift">
<h3><a class="toc-backref" href="#id27">Enabling Glance and Swift</a><a class="headerlink" href="#enabling-glance-and-swift" title="Permalink to this headline">¶</a></h3>
<p>There aren&#8217;t many changes that you will need to make to the default configuration in order to enable Swift to work properly in Swift Compact mode, but you will need to adjust if you want to run Swift on physical partitions</p>
<div class="highlight-python"><pre>...
### GLANCE and SWIFT ###

# Which backend to use for glance
# Supported backends are 'swift' and 'file'
$glance_backend = 'swift'

# Use loopback device for swift:
# options are 'loopback' or 'false'
# This parameter controls where swift partitions are located:
# on physical partitions or inside loopback devices.
$swift_loopback = loopback</pre>
</div>
<p>The default value is <tt class="docutils literal"><span class="pre">loopback</span></tt>, which tells Swift to use a loopback storage device, which is basically a file that acts like a drive, rather than a physical drive.  You can also set this value to <tt class="docutils literal"><span class="pre">false</span></tt>, which tells OpenStack to use a physical drive (or drives) instead.</p>
<div class="highlight-python"><pre># Which IP address to bind swift components to: e.g., which IP swift-proxy should listen on
$swift_local_net_ip = $internal_address

# IP node of controller used during swift installation
# and put into swift configs
$controller_node_public = $internal_virtual_ip

# Hash of proxies hostname|fqdn =&gt; ip mappings.
# This is used by controller_ha.pp manifests for haproxy setup
# of swift_proxy backends
$swift_proxies = $controller_internal_addresses</pre>
</div>
<p>Next, you&#8217;re specifying the <tt class="docutils literal"><span class="pre">swift-master</span></tt>:</p>
<div class="highlight-python"><pre># Set hostname of swift_master.
# It tells on which swift proxy node to build
# *ring.gz files. Other swift proxies/storages
# will rsync them.
if $node[0]['role'] == 'primary-controller' {
  $primary_proxy = true
} else {
  $primary_proxy = false
}
if $node[0]['role'] == 'primary-controller' {
  $primary_controller = true
} else {
  $primary_controller = false
}
$master_swift_proxy_nodes = filter_nodes($nodes,'role','primary-controller')
$master_swift_proxy_ip = $master_swift_proxy_nodes[0]['internal_address']</pre>
</div>
<p>In this case, there&#8217;s no separate <tt class="docutils literal"><span class="pre">fuel-swiftproxy-01</span></tt>, so the master controller will be the primary Swift controller.</p>
</div>
<div class="section" id="configuring-openstack-to-use-syslog">
<h3><a class="toc-backref" href="#id28">Configuring OpenStack to use syslog</a><a class="headerlink" href="#configuring-openstack-to-use-syslog" title="Permalink to this headline">¶</a></h3>
<p>To use the syslog server, adjust the corresponding variables in the <tt class="docutils literal"><span class="pre">if</span> <span class="pre">$use_syslog</span></tt> clause:</p>
<div class="highlight-python"><pre>$use_syslog = true
if $use_syslog {
    class { "::rsyslog::client":
        log_local =&gt; true,
        log_auth_local =&gt; true,
        server =&gt; '127.0.0.1',
        port =&gt; '514'
    }
}</pre>
</div>
<p>For remote logging, use the IP or hostname of the server for the <tt class="docutils literal"><span class="pre">server</span></tt> value and set the <tt class="docutils literal"><span class="pre">port</span></tt> appropriately.  For local logging, <tt class="docutils literal"><span class="pre">set</span> <span class="pre">log_local</span></tt> and <tt class="docutils literal"><span class="pre">log_auth_local</span></tt> to <tt class="docutils literal"><span class="pre">true</span></tt>.</p>
</div>
<div class="section" id="setting-the-version-and-mirror-type">
<h3><a class="toc-backref" href="#id29">Setting the version and mirror type</a><a class="headerlink" href="#setting-the-version-and-mirror-type" title="Permalink to this headline">¶</a></h3>
<p>You can customize the various versions of OpenStack&#8217;s components, though it&#8217;s typical to use the latest versions:</p>
<div class="highlight-python"><pre>### Syslog END ###
case $::osfamily {
    "Debian":  {
       $rabbitmq_version_string = '2.8.7-1'
    }
    "RedHat": {
       $rabbitmq_version_string = '2.8.7-2.el6'
    }
}
# OpenStack packages and customized component versions to be installed.
# Use 'latest' to get the most recent ones or specify exact version if you need to install custom version.
$openstack_version = {
  'keystone'         =&gt; 'latest',
  'glance'           =&gt; 'latest',
  'horizon'          =&gt; 'latest',
  'nova'             =&gt; 'latest',
  'novncproxy'       =&gt; 'latest',
  'cinder'           =&gt; 'latest',
  'rabbitmq_version' =&gt; $rabbitmq_version_string,
}</pre>
</div>
<p>To tell Fuel to download packages from external repos provided by Mirantis and your distribution vendors, make sure the <tt class="docutils literal"><span class="pre">$mirror_type</span></tt> variable is set to <tt class="docutils literal"><span class="pre">default</span></tt>:</p>
<div class="highlight-python"><pre># If you want to set up a local repository, you will need to manually adjust mirantis_repos.pp,
# though it is NOT recommended.
$mirror_type = 'default'
$enable_test_repo = false
$repo_proxy = 'http://10.0.0.100:3128'</pre>
</div>
<p>Once again, the <tt class="docutils literal"><span class="pre">$mirror_type</span></tt> <strong>must</strong> be set to <tt class="docutils literal"><span class="pre">default</span></tt>.  If you set it correctly in <tt class="docutils literal"><span class="pre">config.yaml</span></tt> and ran <tt class="docutils literal"><span class="pre">openstack_system</span></tt> this will already be taken care of.  Otherwise, <strong>make sure</strong> to set this value manually.</p>
<p>Future versions of Fuel will enable you to use your own internal repositories.</p>
</div>
<div class="section" id="setting-verbosity">
<h3><a class="toc-backref" href="#id30">Setting verbosity</a><a class="headerlink" href="#setting-verbosity" title="Permalink to this headline">¶</a></h3>
<p>You also have the option to determine how much information OpenStack provides when performing configuration:</p>
<div class="highlight-python"><pre># This parameter specifies the verbosity level of log messages
# in openstack components config. Currently, it disables or enables debugging.
$verbose = true</pre>
</div>
</div>
<div class="section" id="configuring-rate-limits">
<h3><a class="toc-backref" href="#id31">Configuring Rate-Limits</a><a class="headerlink" href="#configuring-rate-limits" title="Permalink to this headline">¶</a></h3>
<p>Openstack has predefined limits on different HTTP queries for nova-compute and cinder services. Sometimes (e.g. for big clouds or test scenarios) these limits are too strict. (See <a class="reference external" href="http://docs.openstack.org/folsom/openstack-compute/admin/content/configuring-compute-API.html">http://docs.openstack.org/folsom/openstack-compute/admin/content/configuring-compute-API.html</a>.) In this case you can change them to more appropriate values.</p>
<p>There are two hashes describing these limits: <tt class="docutils literal"><span class="pre">$nova_rate_limits</span></tt> and <tt class="docutils literal"><span class="pre">$cinder_rate_limits</span></tt>.</p>
<div class="highlight-python"><pre>#Rate Limits for cinder and Nova
#Cinder and Nova can rate-limit your requests to API services.
#These limits can be reduced for your installation or usage scenario.
#Change the following variables if you want. They are measured in requests per minute.
$nova_rate_limits = {
  'POST' =&gt; 1000,
  'POST_SERVERS' =&gt; 1000,
  'PUT' =&gt; 1000, 'GET' =&gt; 1000,
  'DELETE' =&gt; 1000
}
$cinder_rate_limits = {
  'POST' =&gt; 1000,
  'POST_SERVERS' =&gt; 1000,
  'PUT' =&gt; 1000, 'GET' =&gt; 1000,
  'DELETE' =&gt; 1000
}
...</pre>
</div>
</div>
<div class="section" id="enabling-horizon-https-ssl-mode">
<h3><a class="toc-backref" href="#id32">Enabling Horizon HTTPS/SSL mode</a><a class="headerlink" href="#enabling-horizon-https-ssl-mode" title="Permalink to this headline">¶</a></h3>
<p>Using the <tt class="docutils literal"><span class="pre">$horizon_use_ssl</span></tt> variable, you have the option to decide whether the OpenStack dashboard (Horizon) uses HTTP or HTTPS:</p>
<div class="highlight-python"><pre>...
#  'custom': require fileserver static mount point [ssl_certs] and hostname based certificate existence
$horizon_use_ssl = false</pre>
</div>
<p>This variable accepts the following values:</p>
<blockquote>
<div><ul>
<li><p class="first"><tt class="docutils literal"><span class="pre">false</span></tt>:  In this mode, the dashboard uses HTTP with no encryption.</p>
</li>
<li><p class="first"><tt class="docutils literal"><span class="pre">default</span></tt>:  In this mode, the dashboard uses keys supplied with the standard Apache SSL module package.</p>
</li>
<li><p class="first"><tt class="docutils literal"><span class="pre">exist</span></tt>:  In this case, the dashboard assumes that the domain name-based certificate, or keys, are provisioned in advance.  This can be a certificate signed by any authorized provider, such as Symantec/Verisign, Comodo, GoDaddy, and so on.  The system looks for the keys in these locations:</p>
<dl class="docutils">
<dt>for Debian/Ubuntu:</dt>
<dd><ul class="first last simple">
<li>public  <tt class="docutils literal"><span class="pre">/etc/ssl/certs/domain-name.pem</span></tt></li>
<li>private <tt class="docutils literal"><span class="pre">/etc/ssl/private/domain-name.key</span></tt></li>
</ul>
</dd>
<dt>for Centos/RedHat:</dt>
<dd><ul class="first last simple">
<li>public  <tt class="docutils literal"><span class="pre">/etc/pki/tls/certs/domain-name.crt</span></tt></li>
<li>private <tt class="docutils literal"><span class="pre">/etc/pki/tls/private/domain-name.key</span></tt></li>
</ul>
</dd>
</dl>
</li>
<li><p class="first"><tt class="docutils literal"><span class="pre">custom</span></tt>:  This mode requires a static mount point on the fileserver for <tt class="docutils literal"><span class="pre">[ssl_certs]</span></tt> and certificate pre-existence.  To enable this mode, configure the puppet fileserver by editing <tt class="docutils literal"><span class="pre">/etc/puppet/fileserver.conf</span></tt> to add:</p>
<div class="highlight-python"><pre>[ssl_certs]
  path /etc/puppet/templates/ssl
  allow *</pre>
</div>
<p>From there, create the appropriate directory:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">mkdir</span> <span class="o">-</span><span class="n">p</span> <span class="o">/</span><span class="n">etc</span><span class="o">/</span><span class="n">puppet</span><span class="o">/</span><span class="n">templates</span><span class="o">/</span><span class="n">ssl</span>
</pre></div>
</div>
<p>Add the certificates to this directory.  (Reload the puppetmaster service for these changes to take effect.)</p>
</li>
</ul>
</div></blockquote>
<p>Now we just need to make sure that all of our nodes get the proper values.</p>
</div>
<div class="section" id="defining-the-node-configurations">
<h3><a class="toc-backref" href="#id33">Defining the node configurations</a><a class="headerlink" href="#defining-the-node-configurations" title="Permalink to this headline">¶</a></h3>
<p>Now that we&#8217;ve set all of the global values, its time to make sure that the actual node definitions are correct. For example, by default all nodes will enable Cinder on <tt class="docutils literal"><span class="pre">/dev/sdb</span></tt>.  If you don&#8217;t want to enable Cinder on all controllers set <tt class="docutils literal"><span class="pre">nv_physical_volume</span></tt> to <tt class="docutils literal"><span class="pre">null</span></tt> for a specific node or nodes.</p>
<div class="highlight-python"><pre>...
class compact_controller (
  $quantum_network_node = $quantum_netnode_on_cnt
) {
  class { 'openstack::controller_ha':
    controller_public_addresses   =&gt; $controller_public_addresses,
    controller_internal_addresses =&gt; $controller_internal_addresses,
    internal_address        =&gt; $internal_address,
    public_interface        =&gt; $public_int,
    internal_interface      =&gt; $internal_int,
 ...
    use_unicast_corosync    =&gt; $use_unicast_corosync,
    ha_provider             =&gt; $ha_provider
  }
  class { 'swift::keystone::auth':
    password         =&gt; $swift_user_password,
    public_address   =&gt; $public_virtual_ip,
    internal_address =&gt; $internal_virtual_ip,
    admin_address    =&gt; $internal_virtual_ip,
  }
}
...</pre>
</div>
<p>To reduce repeated manual configuration, Fuel includes a class for the controllers. This eliminates the need to make global changes for each individual controller.  You will note that lower down in this configuration segment that this class also lets you specify the individual controllers and compute nodes:</p>
<div class="highlight-python"><pre>...
    node /fuel-controller-[\d+]/ {
      include stdlib
      class { 'operatingsystem::checksupported':
          stage =&gt; 'setup'
      }

      class {'::node_netconfig':
          mgmt_ipaddr    =&gt; $::internal_address,
          mgmt_netmask   =&gt; $::internal_netmask,
          public_ipaddr  =&gt; $::public_address,
          public_netmask =&gt; $::public_netmask,
          stage          =&gt; 'netconfig',
      }

      class {'nagios':
        proj_name       =&gt; $proj_name,
        services        =&gt; [
          'host-alive','nova-novncproxy','keystone', 'nova-scheduler',
          'nova-consoleauth', 'nova-cert', 'haproxy', 'nova-api', 'glance-api',
          'glance-registry','horizon', 'rabbitmq', 'mysql', 'swift-proxy',
          'swift-account', 'swift-container', 'swift-object',
        ],
        whitelist       =&gt; ['127.0.0.1', $nagios_master],
        hostgroup       =&gt; 'controller',
      }

      class { compact_controller: }
      $swift_zone = $node[0]['swift_zone']

      class { 'openstack::swift::storage_node':
        storage_type       =&gt; $swift_loopback,
        swift_zone         =&gt; $swift_zone,
        swift_local_net_ip =&gt; $internal_address,
      }

      class { 'openstack::swift::proxy':
        swift_user_password     =&gt; $swift_user_password,
        swift_proxies           =&gt; $swift_proxies,
        ...
        rabbit_ha_virtual_ip      =&gt; $internal_virtual_ip,
      }
    }</pre>
</div>
<p>Note that each controller has the swift_zone specified, so each of the three controllers can represent each of the three Swift zones.
Similarly, site.pp defines a class for the compute nodes.</p>
</div>
<div class="section" id="installing-nagios-monitoring-using-puppet">
<h3><a class="toc-backref" href="#id34">Installing Nagios Monitoring using Puppet</a><a class="headerlink" href="#installing-nagios-monitoring-using-puppet" title="Permalink to this headline">¶</a></h3>
<p>Fuel provides a way to deploy Nagios for monitoring your OpenStack cluster. Nagios is an open source distributed management and monitoring infrastructure that is commonly used in data centers to keep an eye on thousands of servers. Nagios requires the installation of a software agent on all nodes, as well as having a master server for Nagios which will collect and display all the results. The agent, the Nagios NRPE addon, allows OpenStack to execute Nagios plugins on remote Linux/Unix machines. The main reason for doing this is to monitor key resources (such as CPU load, memory usage, etc.), as well as provide more advanced metrics and performance data on local and remote machines.</p>
<div class="section" id="nagios-agent">
<h4><a class="toc-backref" href="#id35">Nagios Agent</a><a class="headerlink" href="#nagios-agent" title="Permalink to this headline">¶</a></h4>
<p>In order to install Nagios NRPE on a compute or controller node, a node should have the following settings:</p>
<div class="highlight-python"><pre>class {'nagios':
  proj_name       =&gt; 'test',
  services        =&gt; ['nova-compute','nova-network','libvirt'],
  whitelist       =&gt; ['127.0.0.1', $nagios_master],
  hostgroup       =&gt; 'compute',
}</pre>
</div>
<ul class="simple">
<li><tt class="docutils literal"><span class="pre">proj_name</span></tt>: An environment for nagios commands and the directory (<tt class="docutils literal"><span class="pre">/etc/nagios/test/</span></tt>).</li>
<li><tt class="docutils literal"><span class="pre">services</span></tt>: All services to be monitored by nagios.</li>
<li><tt class="docutils literal"><span class="pre">whitelist</span></tt>: The array of IP addreses trusted by NRPE.</li>
<li><tt class="docutils literal"><span class="pre">hostgroup</span></tt>: The group to be used in the nagios master (do not forget create the group in the nagios master).</li>
</ul>
</div>
<div class="section" id="nagios-server">
<h4><a class="toc-backref" href="#id36">Nagios Server</a><a class="headerlink" href="#nagios-server" title="Permalink to this headline">¶</a></h4>
<p>In order to install Nagios Master on any convenient node, a node should have the following applied:</p>
<div class="highlight-python"><pre>class {'nagios::master':
  proj_name       =&gt; 'test',
  templatehost    =&gt; {'name' =&gt; 'default-host','check_interval' =&gt; '10'},
  templateservice =&gt; {'name' =&gt; 'default-service' ,'check_interval'=&gt;'10'},
  hostgroups      =&gt; ['compute','controller'],
  contactgroups   =&gt; {'group' =&gt; 'admins', 'alias' =&gt; 'Admins'},
  contacts        =&gt; {'user' =&gt; 'hotkey', 'alias' =&gt; 'Dennis Hoppe',
               'email' =&gt; 'nagios@%{domain}',
               'group' =&gt; 'admins'},
}</pre>
</div>
<ul class="simple">
<li><tt class="docutils literal"><span class="pre">proj_name</span></tt>: The environment for nagios commands and the directory (<tt class="docutils literal"><span class="pre">/etc/nagios/test/</span></tt>).</li>
<li><tt class="docutils literal"><span class="pre">templatehost</span></tt>: The group of checks and intervals parameters for hosts (as a Hash).</li>
<li><tt class="docutils literal"><span class="pre">templateservice</span></tt>: The group of checks and intervals parameters for services  (as a Hash).</li>
<li><tt class="docutils literal"><span class="pre">hostgroups</span></tt>: All groups which on NRPE nodes (as an Array).</li>
<li><tt class="docutils literal"><span class="pre">contactgroups</span></tt>: The group of contacts (as a Hash).</li>
<li><tt class="docutils literal"><span class="pre">contacts</span></tt>: Contacts to receive error reports (as a Hash)</li>
</ul>
</div>
<div class="section" id="health-checks">
<h4><a class="toc-backref" href="#id37">Health Checks</a><a class="headerlink" href="#health-checks" title="Permalink to this headline">¶</a></h4>
<p>You can see the complete definition of the available services to monitor and their health checks at <tt class="docutils literal"><span class="pre">deployment/puppet/nagios/manifests/params.pp</span></tt>.</p>
<p>Here is the list:</p>
<div class="highlight-python"><pre>$services_list = {
  'nova-compute' =&gt; 'check_nrpe_1arg!check_nova_compute',
  'nova-network' =&gt; 'check_nrpe_1arg!check_nova_network',
  'libvirt' =&gt; 'check_nrpe_1arg!check_libvirt',
  'swift-proxy' =&gt; 'check_nrpe_1arg!check_swift_proxy',
  'swift-account' =&gt; 'check_nrpe_1arg!check_swift_account',
  'swift-container' =&gt; 'check_nrpe_1arg!check_swift_container',
  'swift-object' =&gt; 'check_nrpe_1arg!check_swift_object',
  'swift-ring' =&gt; 'check_nrpe_1arg!check_swift_ring',
  'keystone' =&gt; 'check_http_api!5000',
  'nova-novncproxy' =&gt; 'check_nrpe_1arg!check_nova_novncproxy',
  'nova-scheduler' =&gt; 'check_nrpe_1arg!check_nova_scheduler',
  'nova-consoleauth' =&gt; 'check_nrpe_1arg!check_nova_consoleauth',
  'nova-cert' =&gt; 'check_nrpe_1arg!check_nova_cert',
  'cinder-scheduler' =&gt; 'check_nrpe_1arg!check_cinder_scheduler',
  'cinder-volume' =&gt; 'check_nrpe_1arg!check_cinder_volume',
  'haproxy' =&gt; 'check_nrpe_1arg!check_haproxy',
  'memcached' =&gt; 'check_nrpe_1arg!check_memcached',
  'nova-api' =&gt; 'check_http_api!8774',
  'cinder-api' =&gt; 'check_http_api!8776',
  'glance-api' =&gt; 'check_http_api!9292',
  'glance-registry' =&gt; 'check_nrpe_1arg!check_glance_registry',
  'horizon' =&gt; 'check_http_api!80',
  'rabbitmq' =&gt; 'check_rabbitmq',
  'mysql' =&gt; 'check_galera_mysql',
  'apt' =&gt; 'nrpe_check_apt',
  'kernel' =&gt; 'nrpe_check_kernel',
  'libs' =&gt; 'nrpe_check_libs',
  'load' =&gt; 'nrpe_check_load!5.0!4.0!3.0!10.0!6.0!4.0',
  'procs' =&gt; 'nrpe_check_procs!250!400',
  'zombie' =&gt; 'nrpe_check_procs_zombie!5!10',
  'swap' =&gt; 'nrpe_check_swap!20%!10%',
  'user' =&gt; 'nrpe_check_users!5!10',
  'host-alive' =&gt; 'check-host-alive',
}</pre>
</div>
</div>
</div>
<div class="section" id="node-definitions">
<h3><a class="toc-backref" href="#id38">Node definitions</a><a class="headerlink" href="#node-definitions" title="Permalink to this headline">¶</a></h3>
<p>The following is a list of the node definitions generated for a Compact HA deployment.  Other deployment configurations generate other definitions.  For example, the <tt class="docutils literal"><span class="pre">openstack/examples/site_openstack_full.pp</span></tt> template specifies the following nodes:</p>
<ul class="simple">
<li>fuel-controller-01</li>
<li>fuel-controller-02</li>
<li>fuel-controller-03</li>
<li>fuel-compute-[d+]</li>
<li>fuel-swift-01</li>
<li>fuel-swift-02</li>
<li>fuel-swift-03</li>
<li>fuel-swiftproxy-[d+]</li>
<li>fuel-quantum</li>
</ul>
<p>Using this architecture, the system includes three stand-alone swift-storage servers, and one or more swift-proxy servers.</p>
<p>With <tt class="docutils literal"><span class="pre">site.pp</span></tt> prepared, you&#8217;re ready to perform the actual installation.</p>
</div>
</div>
<div class="section" id="deploying-openstack">
<h2><a class="toc-backref" href="#id39">Deploying OpenStack</a><a class="headerlink" href="#deploying-openstack" title="Permalink to this headline">¶</a></h2>
<p>You have two options for deploying OpenStack.  The best method is to use orchestration, but you can also deploy your nodes manually.</p>
<div class="section" id="deploying-via-orchestration">
<span id="orchestration"></span><h3><a class="toc-backref" href="#id40">Deploying via orchestration</a><a class="headerlink" href="#deploying-via-orchestration" title="Permalink to this headline">¶</a></h3>
<p>Performing a small series of manual installs many be an acceptable approach in some circumstances, but if you plan on deploying to a large number of servers then we strongly suggest that you consider orchestration. You can perform a deployment using orchestration with Fuel using the &#8220;astute&#8221; script. This script is configured using the &#8220;astute.yaml&#8221; file that was created when you ran &#8220;openstack_system&#8221; earlier in this process.</p>
<p>To confirm that your servers are ready for orchestration, execute the following command:</p>
<div class="highlight-python"><pre>mco ping</pre>
</div>
<p>You should see all three controllers, plus the compute node, in the response to the command:</p>
<div class="highlight-python"><pre>fuel-compute-01                                    time=107.26 ms
fuel-controller-01                                 time=120.14 ms
fuel-controller-02                                 time=135.94 ms
fuel-controller-03                                 time=139.33 ms</pre>
</div>
<p>To run the orchestrator, log in to <tt class="docutils literal"><span class="pre">fuel-pm</span></tt> and execute:</p>
<div class="highlight-python"><pre>astute -f astute.yaml</pre>
</div>
<p>You will see a message on <tt class="docutils literal"><span class="pre">fuel-pm</span></tt> stating that the installation has started on fuel-controller-01.  To see what&#8217;s going on on the target node on Ubuntu-based OS, enter this command:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">tail</span> <span class="o">-</span><span class="n">f</span> <span class="o">/</span><span class="n">var</span><span class="o">/</span><span class="n">log</span><span class="o">/</span><span class="n">syslog</span>
</pre></div>
</div>
<p>for CentOS or RedHat use this command:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">tail</span> <span class="o">-</span><span class="n">f</span> <span class="o">/</span><span class="n">var</span><span class="o">/</span><span class="n">log</span><span class="o">/</span><span class="n">messages</span>
</pre></div>
</div>
<p>Note that Puppet will require several runs to install all the different roles. The first time it runs, the orchestrator will show an error. This error means that the installation isn&#8217;t complete, but will be rectified after the various rounds of installation are completed.  Also, after the first run on each server, the orchestrator doesn&#8217;t output messages on fuel-pm; when it&#8217;s finished running, it will return you to the command prompt.  In the meantime, you can see what&#8217;s going on by watching the logs on each individual machine.</p>
</div>
<div class="section" id="installing-openstack-using-puppet-directly">
<h3><a class="toc-backref" href="#id41">Installing OpenStack using Puppet directly</a><a class="headerlink" href="#installing-openstack-using-puppet-directly" title="Permalink to this headline">¶</a></h3>
<p>If, for some reason, you choose not to use orchestration &#8211; one common example it when adding a single node to an existing (non-HA) cluster &#8211; you have the option to install on one or more nodes using Puppet.</p>
<p>Start by logging in to the target server &#8211; fuel-controller-01 to start, if you&#8217;re starting from scratch &#8211; and running the Puppet agent.</p>
<p>One optional step would be to use the script command to log all of your output so you can check for errors:</p>
<div class="highlight-python"><pre>script agent-01.log
puppet agent --test</pre>
</div>
<p>You will to see a great number of messages scroll by, and the installation will take a significant amount of time. When the process has completed, press CTRL-D to stop logging and grep for errors:</p>
<div class="highlight-python"><pre>grep err: agent-01.log</pre>
</div>
<p>If you find any errors relating to other nodes you may safely ignore them for now.</p>
<p>Now you can run the same installation procedure on fuel-controller-02 and fuel-controller-03, as well as fuel-compute-01.</p>
<p>Note that the controllers must be installed sequentially due to the nature of assembling a MySQL cluster based on Galera. This means that one server must complete its installation before the next installation is started. However, compute nodes can be installed concurrently once the controllers are in place.</p>
<p>In some cases, you may find errors related to resources that are not yet available when the installation takes place. To solve that problem, simply re-run the puppet agent on the affected node after running the other controllers, and again grep for error messages. When you see no errors on any of your nodes, your OpenStack cluster is ready to go.</p>
</div>
<div class="section" id="examples-of-openstack-installation-sequences">
<h3><a class="toc-backref" href="#id42">Examples of OpenStack installation sequences</a><a class="headerlink" href="#examples-of-openstack-installation-sequences" title="Permalink to this headline">¶</a></h3>
<p>When running Puppet manually, the exact sequence depends on the configuration goals you&#8217;re trying to achieve.  In most cases, you&#8217;ll need to run Puppet more than once; with every pass Puppet collects and adds necessary absent information to the OpenStack configuration, stores it to PuppedDB and applies necessary changes.</p>
<blockquote>
<div><p><strong>Note:</strong> <em>Sequentially run</em> means you don&#8217;t start the next node deployment until previous one is finished.</p>
<p><strong>Example 1:</strong> <strong>Full OpenStack deployment with standalone storage nodes</strong></p>
<blockquote>
<div><ul class="simple">
<li>Create necessary volumes on storage nodes as described in  <a class="reference internal" href="frequently-asked-questions/0070-common-technical-issues.html#create-the-xfs-partition"><em>Creating the XFS partition</em></a>.</li>
<li>Sequentially run a deployment pass on every SwiftProxy node (<tt class="docutils literal"><span class="pre">fuel-swiftproxy-01</span> <span class="pre">...</span> <span class="pre">fuel-swiftproxy-xx</span></tt>), starting with the <tt class="docutils literal"><span class="pre">primary-swift-proxy</span> <span class="pre">node</span></tt>. Node names are set by the <tt class="docutils literal"><span class="pre">$swift_proxies</span></tt> variable in <tt class="docutils literal"><span class="pre">site.pp</span></tt>. There are 2 Swift Proxies by default.</li>
<li>Sequentially run a deployment pass on every storage node (<tt class="docutils literal"><span class="pre">fuel-swift-01</span></tt> ... <tt class="docutils literal"><span class="pre">fuel-swift-xx</span></tt>).</li>
<li>Sequentially run a deployment pass on the controller nodes (<tt class="docutils literal"><span class="pre">fuel-controller-01</span> <span class="pre">...</span> <span class="pre">fuel-controller-xx</span></tt>). starting with the <tt class="docutils literal"><span class="pre">primary-controller</span></tt> node.</li>
<li>Run a deployment pass on the Neutron (formerly Quantum) node (<tt class="docutils literal"><span class="pre">fuel-quantum</span></tt>) to install the Neutron router.</li>
<li>Run a deployment pass on every compute node (<tt class="docutils literal"><span class="pre">fuel-compute-01</span> <span class="pre">...</span> <span class="pre">fuel-compute-xx</span></tt>) - unlike the controllers, these nodes may be deployed in parallel.</li>
<li>Run an additional deployment pass on Controller 1 only (<tt class="docutils literal"><span class="pre">fuel-controller-01</span></tt>) to finalize the Galera cluster configuration.</li>
</ul>
</div></blockquote>
<p><strong>Example 2:</strong> <strong>Compact OpenStack deployment with storage and swift-proxy combined with nova-controller on the same nodes</strong></p>
<blockquote>
<div><ul class="simple">
<li>Create the necessary volumes on controller nodes as described in <a class="reference internal" href="frequently-asked-questions/0070-common-technical-issues.html#create-the-xfs-partition"><em>Creating the XFS partition</em></a></li>
<li>Sequentially run a deployment pass on the controller nodes (<tt class="docutils literal"><span class="pre">fuel-controller-01</span> <span class="pre">...</span> <span class="pre">fuel-controller-xx</span></tt>), starting with the <tt class="docutils literal"><span class="pre">primary-controller</span> <span class="pre">node</span></tt>. Errors in Swift storage such as <em>/Stage[main]/Swift::Storage::Container/Ring_container_device[&lt;device address&gt;]: Could not evaluate: Device not found check device on &lt;device address&gt;</em> are expected during the deployment passes until the very final pass.</li>
<li>Run an additional deployment pass on Controller 1 only (<tt class="docutils literal"><span class="pre">fuel-controller-01</span></tt>) to finalize the Galera cluster configuration.</li>
<li>Run a deployment pass on the Neutron node (<tt class="docutils literal"><span class="pre">fuel-quantum</span></tt>) to install the Neutron router.</li>
<li>Run a deployment pass on every compute node (<tt class="docutils literal"><span class="pre">fuel-compute-01</span> <span class="pre">...</span> <span class="pre">fuel-compute-xx</span></tt>) - unlike the controllers these nodes may be deployed in parallel.</li>
</ul>
</div></blockquote>
<p><strong>Example 3:</strong> <strong>OpenStack HA installation without Swift</strong></p>
<blockquote>
<div><ul class="simple">
<li>Sequentially run a deployment pass on the controller nodes (<tt class="docutils literal"><span class="pre">fuel-controller-01</span> <span class="pre">...</span> <span class="pre">fuel-controller-xx</span></tt>), starting with the primary controller. No errors should appear during this deployment pass.</li>
<li>Run an additional deployment pass on the primary controller only (<tt class="docutils literal"><span class="pre">fuel-controller-01</span></tt>) to finalize the Galera cluster configuration.</li>
<li>Run a deployment pass on the Neutron node (<tt class="docutils literal"><span class="pre">fuel-quantum</span></tt>) to install the Neutron router.</li>
<li>Run a deployment pass on every compute node (<tt class="docutils literal"><span class="pre">fuel-compute-01</span> <span class="pre">...</span> <span class="pre">fuel-compute-xx</span></tt>) - unlike the controllers these nodes may be deployed in parallel.</li>
</ul>
</div></blockquote>
<p><strong>Example 4:</strong> <strong>The simplest OpenStack installation: Controller + Compute on the same node</strong></p>
<blockquote>
<div><ul class="simple">
<li>Set the <tt class="docutils literal"><span class="pre">node</span> <span class="pre">/fuel-controller-[\d+]/</span></tt> variable in <tt class="docutils literal"><span class="pre">site.pp</span></tt> to match the hostname of the node on which you are going to deploy OpenStack. Set the <tt class="docutils literal"><span class="pre">node</span> <span class="pre">/fuel-compute-[\d+]/</span></tt> variable to <strong>mismatch</strong> the node name. Run a deployment pass on this node. No errors should appear during this deployment pass.</li>
<li>Set the <tt class="docutils literal"><span class="pre">node</span> <span class="pre">/fuel-compute-[\d+]/</span></tt> variable in <tt class="docutils literal"><span class="pre">site.pp</span></tt> to match the hostname of the node on which you are going to deploy OpenStack. Set the <tt class="docutils literal"><span class="pre">node</span> <span class="pre">/fuel-controller-[\d+]/</span></tt> variable to <strong>mismatch</strong> the node name. Run a deployment pass on this node. No errors should appear during this deployment pass.</li>
</ul>
</div></blockquote>
</div></blockquote>
</div>
</div>
<div class="section" id="testing-openstack">
<h2><a class="toc-backref" href="#id43">Testing OpenStack</a><a class="headerlink" href="#testing-openstack" title="Permalink to this headline">¶</a></h2>
<p>Now that you&#8217;ve installed OpenStack, its time to take your new openstack cloud for a drive around the block. Follow these steps:</p>
<ol class="arabic simple">
<li>On the host machine, open your browser to</li>
</ol>
<blockquote>
<div><p><a class="reference external" href="http://192.168.0.10/">http://192.168.0.10/</a>  (Adjust this value to your own <tt class="docutils literal"><span class="pre">public_virtual_ip</span></tt>.)</p>
<p>and login as nova/nova (unless you changed this information in <tt class="docutils literal"><span class="pre">site.pp</span></tt>)</p>
</div></blockquote>
<ol class="arabic simple">
<li>Click the Project tab in the left-hand column.</li>
<li>Under Manage Compute, choose Access &amp; Security to set security settings:</li>
</ol>
<blockquote>
<div><ol class="arabic simple">
<li>Click Create Keypair and enter a name for the new keypair.  The private key should download automatically; make sure to keep it safe.</li>
<li>Click Access &amp; Security again and click Edit Rules for the default Security Group.  Add a new rule allowing TCP connections from port 22 to port 22 for all IP addresses using a CIDR of 0.0.0.0/0.  (You can also customize this setting as necessary.)  Click Add Rule to save the new rule.</li>
<li>Add a second new rule allowing ICMP connections with a type and code of -1 to the default Security Group and click Add Rule to save.</li>
</ol>
</div></blockquote>
<ol class="arabic simple">
<li>Click Allocate IP To Project and add two new floating ips.  Notice that they come from the pool specified in <tt class="docutils literal"><span class="pre">config.yaml</span></tt> and <tt class="docutils literal"><span class="pre">site.pp</span></tt>.</li>
<li>Click Images &amp; Snapshots, then Create Image.  Enter a name and specify the Image Location as <a class="reference external" href="https://launchpad.net/cirros/trunk/0.3.0/+download/cirros-0.3.0-x86_64-disk.img">https://launchpad.net/cirros/trunk/0.3.0/+download/cirros-0.3.0-x86_64-disk.img</a>, with a Format of QCOW2.  Check the Public checkbox.</li>
<li>The next step is to upload an image to use for creating VMs, but an OpenStack bug prevents you from doing this in the browser. Instead, log in to any of the controllers as root and execute the following commands:</li>
</ol>
<blockquote>
<div>cd ~
source openrc
glance image-create &#8211;name cirros &#8211;container-format bare &#8211;disk-format qcow2 &#8211;is-public yes &#8211;location <a class="reference external" href="https://launchpad.net/cirros/trunk/0.3.0/+download/cirros-0.3.0-x86_64-disk.img">https://launchpad.net/cirros/trunk/0.3.0/+download/cirros-0.3.0-x86_64-disk.img</a></div></blockquote>
<ol class="arabic simple">
<li>Go back to the browser and refresh the page.  Launch a new instance of this image
using the tiny flavor.  Click the Networking tab and choose the default <tt class="docutils literal"><span class="pre">net04_ext</span></tt> network, then click the Launch button.</li>
<li>On the instances page:</li>
</ol>
<blockquote>
<div><ol class="arabic simple">
<li>Click the new instance and look at the settings.</li>
<li>Click the Logs tab to look at the logs.</li>
<li>Click the VNC tab to log in. If you see just a big black rectangle, the machine is in screensaver mode; click the grey area and press the space bar to wake it up, then login as <tt class="docutils literal"><span class="pre">cirros/cubswin:)</span></tt>.</li>
<li>At the command line, enter <tt class="docutils literal"><span class="pre">ifconfig</span> <span class="pre">-a</span> <span class="pre">|</span> <span class="pre">more</span></tt> and see the assigned ip address.</li>
<li>Enter <tt class="docutils literal"><span class="pre">sudo</span> <span class="pre">fdisk</span> <span class="pre">-l</span></tt> to see that no volume has yet been assigned to this VM.</li>
</ol>
</div></blockquote>
<ol class="arabic simple">
<li>On the Instances page, click Assign Floating IP and assign an IP address to your instance.  You can either choose from one of the existing created IPs by using the pulldown menu or click the plus sign (+) to choose a network and allocate a new IP address.</li>
</ol>
<blockquote>
<div><ol class="arabic simple">
<li>From your host machine, ping the floating ip assigned to this VM.</li>
<li>If that works, try to <tt class="docutils literal"><span class="pre">ssh</span> <span class="pre">cirros&#64;floating-ip</span></tt> from the host machine.</li>
</ol>
</div></blockquote>
<ol class="arabic simple">
<li>Back in the browser, click Volumes and Create Volume.  Create the new volume, and attach it to the instance.</li>
<li>Go back to the VNC tab and repeat <tt class="docutils literal"><span class="pre">fdisk</span> <span class="pre">-l</span></tt> to see the new unpartitioned disk attached.</li>
</ol>
<p>Now your new VM is ready to be used.</p>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="0055-production-considerations.html" title="Production Considerations"
             >next</a> |</li>
        <li class="right" >
          <a href="0045-installation-fuel-web.html" title="Create a multi-node OpenStack cluster using Fuel Web"
             >previous</a> |</li>
        <li><a href="../index.html">Fuel for OpenStack 3.0 documentation</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2013, Mirantis.
      Last updated on 2013/07/22.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.2b1.
    </div>
  </body>
</html>